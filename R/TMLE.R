## Provide onestep and TMLE estimators under nonparametrically saturated model ====
#' Estimate average counterfactual outcome E(Y(a)).
#'
#' Function for estimating the average counterfactual outcome E(Y(a)) under a nonparametrically saturated model.
#' @param a Treatment level or a length two vector specifying treatment levels.
#'          The average counterfactual outcome(s) will be computed at the specified treatment level(s).
#'          If `a` is a single value, the function will return `E(Y(a))`, which is the
#'          average counterfactual outcome under the treatment level specified by `a`.
#'          If `a` is a vector of length two, say `c(value1, value2)`, the function will return
#'          `E(Y(a=value1)) - E(Y(a=value2))`, which is the contrast between the average
#'          counterfactual outcomes under the two specified treatment levels.
#' @param data A Dataframe contains all the variables listed in vertices parameter
#' @param vertices A vector of variable names in the causal graph
#' @param di_edges A list of directed edges in the causal graph. For example, `di_edges=list(c('A','B'))` means there is a directed edge from vertex A to B.
#' @param bi_edges A list of bidirected edges in the causal graph. For example, `bi_edges=list(c('A','B'))` means there is a bidirected edge between vertex A and B.
#' @param multivariate.variables A list of variables that are multivariate in the causal graph. For example, `multivariate.variables=list(M=c('M1,'M2'))` means M is bivariate and the corresponding columns in the dataframe are M1 and M2.
#' @param graph A graph object generated by the `make.graph()` function. User only need to specify either `graph` or `vertices`, `di_edges`, `bi_edges`, and `multivariate.variables`.
#' @param treatment A character string indicating the treatment variable
#' @param outcome A character string indicating the outcome variable
#' @param superlearner.seq A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted when performing sequential regression to estimate the conditional expectations as the nuisances consititute the efficient influence function.
#' @param superlearner.Y A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the outcome regression.
#' @param superlearner.A A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the propensity score.
#' @param superlearner.M A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the density ratio for variables in set M when `ratio.method.M="bayes"`.
#' @param superlearner.L A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the density ratio for variables in set L when `ratio.method.L="bayes"`.
#' @param crossfit A logical indicator determines whether crossfitting is adopted for SuperLearner. If crossfit is set to TRUE, the data is split into K folds as specified by the `K` parameter.
#' @param K An integer specifying the number of folds for crossfitting.
#' @param ratio.method.L A character string indicating the method used to estimate the density ratio associated with L. There are three options: 'bayes', 'dnorm', and 'densratio'.
#' The default is 'bayes'. The `bayes` method estimates the density ratio \eqn{p(L|...,A=a0)/p(L|...,A=a1)} by rewriting it as \eqn{(p(a0|L,...)/p(a1|L,...))/(p(a0|...)/p(a1|...))}.
#' Here \eqn{p(a0|L,...)} and \eqn{p(a0|...)} are estimated via linear regression if `superlearner.L=F` and via superlearner if `superlearner.L=T`.
#' The `dnorm` method estimates the density ratio \eqn{p(L|...,A=a0)/p(L|...,A=a1)} by assuming that the density of L given A is normal if L is continuous. And assume L|...,A can be modeled via logistic regression if L is binary.
#' The `densratio` method estimates the density ratio \eqn{p(L|...,A=a0)/p(L|...,A=a1)} by using the `densratio` function in the \link[densratio]{densratio} package. The `densratio` method is computationally expensive compared to the other two methods.
#' Therefore, if there is a large number of variables in your graph, it is recommended to use either `dnorm` or `bayes` method instead.
#' @details In assuming normal distribution. The `dnorm` method estimates the mean of the normal distribution by fitting a linear regression model with only linear terms and no interaction terms and the variance of the normal distribution sample variance of the error term resulted from the linear regression model.
#' In assuming logistic regression, the `dnorm` method further assume the logistic regression contains only linear terms and no interaction terms.
#' @param ratio.method.M A character string indicating the method used to estimate the density ratio associated with M. There are three options: 'bayes', 'dnorm', and 'densratio'.
#' The default is 'bayes'. The "bayes" method estimates the density ratio \eqn{p(M|...,A=a0)/p(M|...,A=a1)} by rewriting it as \eqn{(p(a0|M,...)/p(a1|M,...))/(p(a0|...)/p(a1|...))}.
#' Here \eqn{p(a0|M,...)} and \eqn{p(a0|...)} are estimated via linear regression if `superlearner.M=F` and via superlearner if `superlearner.M=T`.
#' @param lib.seq A character vector specifying the library of algorithms to be used in the SuperLearner for sequential regression.
#' @param lib.L A character vector specifying the library of algorithms to be used in the SuperLearner for estimating the density ratio associated with L.
#' @param lib.M A character vector specifying the library of algorithms to be used in the SuperLearner for estimating the density ratio associated with M.
#' @param lib.Y A character vector specifying the library of algorithms to be used in the SuperLearner for outcome regression.
#' @param lib.A A character vector specifying the library of algorithms to be used in the SuperLearner for propensity score estimation.
#' @param formulaY Regression formula for the outcome regression of Y on it's Markov pillow. The default is 'Y ~ .'.
#' @param formulaA Regression formula for the propensity score regression of A on it's Markov pillow. The default is 'A ~ .'.
#' @param linkY_binary The link function used for outcome regression of Y on it's Markov pillow when Y is binary and superlearner is not sued. The default is the 'logit' link.
#' @param linkA The link function used for propensity score regression of A on it's Markov pillow if superlearner is not used. The default is the 'logit' link.
#' @param cvg.criteria A numerical value representing the convergence criteria for the iterative update of the nusiances in TMLE.
#' If the absolute of the mean of efficient influence function is less than cvg.criteria, the iterative update stops. The default value is 0.01.
#' @param n.iter The maximum number of iterations for the iterative update of the nuisances in TMLE. The default value is 500.
#' @param truncate_lower The lower bound for truncation of the propensity score. The default is 0, which means no truncation.
#' @param truncate_upper The upper bound for truncation of the propensity score. The default is 1, which means no truncation.
#' @param zerodiv.avoid A numerical threshold to avoid division by zero in the estimation of the density ratio. The default is 0, which means no threshold.
#' @return Function outputs a list containing TMLE results and onestep results:
#' \describe{
#'       \item{\code{estimated_psi}}{The estimated parameter of interest: \eqn{E(Y^a)}}
#'       \item{\code{lower.ci}}{Lower bound of the 95% confidence interval for \code{estimated_psi}}
#'       \item{\code{upper.ci}}{Upper bound of the 95 pct confidence interval for \code{estimated_psi}}
#'       \item{\code{EIF}}{The estimated efficient influence function evaluated at the observed data}
#'       \item{\code{EIF.Y}}{The part of the EIF corresponding to the outcome regression}
#'       \item{\code{EIF.A}}{The part of EIF corresponding to the propensity score regression}
#'       \item{\code{EIF.v}}{The part of EIF corresponding to variables that between treatment and outcome according to the topological order of vertices in the graph.}
#'       \item{\code{p.a1.mpA}}{The estimated propensity score for treatment level 1-a given the Markov pillow of A}
#'       \item{\code{mu.next.A}}{Estimated \eqn{E[v|mp(v)]} for v that comes right after A in topological order}
#'       \item{\code{EDstar}}{The sample mean of the estimated efficient influence function. If convergence is achieved, this should be close to 0 for TMLE estimators.}
#'.      \item{\code{EDstar.record}}{A vector recording the value of EDstar over iterations.}
#'       \item{\code{iter}}{Number of iterations where convergence is achieved for the iterative update of the mediator density and propensity score.}}
#' @examples
#' # E(Y(1)) estimation.
#' ADMGtmle(a=1,data=data_fig_4a, vertices=c('A','M','L','Y','X'),
#' bi_edges=list(c('A','Y')), di_edges=list(c('X','A'), c('X','M'),
#' c('X','L'),c('X','Y'), c('M','Y'), c('A','M'), c('A','L'), c('M','L'), c('L','Y')),
#' treatment='A', outcome='Y', multivariate.variables = list(M=c('M.1','M.2')))
#' @import dplyr MASS densratio SuperLearner mvtnorm stats itertools utils
#' @export
#'
#'
ADMGtmle <- function(a=NULL,data=NULL,vertices=NULL, di_edges=NULL, bi_edges=NULL, treatment=NULL, outcome=NULL, multivariate.variables=NULL, graph=NULL,
                 superlearner.seq = F, # whether run superlearner for sequential regression
                 superlearner.Y=F, # whether run superlearner for outcome regression
                 superlearner.A=F, # whether run superlearner for propensity score
                 superlearner.M=F, # whether run superlearner for estimating densratio for M using bayes method
                 superlearner.L=F, # whether run superlearner for estimating densratio for L using bayes method
                 crossfit=F, K=5,
                 ratio.method.L="bayes", # method for estimating the density ratio associated with M
                 ratio.method.M="bayes", # method for estimating the density ratio associated with L
                 lib.seq = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for sequential regression
                 lib.L = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for density ratio estimation via bayes rule for variables in L
                 lib.M = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for density ratio estimation via bayes rule for variables in M
                 lib.Y = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for outcome regression
                 lib.A = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for propensity score
                 formulaY="Y ~ .", formulaA="A ~ .", # regression formula for outcome regression and propensity score if superlearner is not used
                 linkY_binary="logit", linkA="logit", # link function for outcome regression and propensity score if superlearner is not used
                 n.iter=500, cvg.criteria=0.01,
                 truncate_lower=0, truncate_upper=1, zerodiv.avoid=0){

  n <- nrow(data)

  # make a graph object if it's not provided
  if (is.null(graph)){ graph <- make.graph(vertices=vertices, bi_edges=bi_edges, di_edges=di_edges, multivariate.variables=multivariate.variables)}


  #####################################################
  # Fixable vs Primal fixable
  #####################################################


  if (is.fix(graph, treatment)){ # if the graph is fixable

    print("The treatment is fixable. Estimation provided via backdoor adjustment.")


    if (is.vector(a) & length(a)>2){ ## Invalid input ==

      print("Invalid input. Enter a=c(vaule1,value2) for average causal effect estimation: (Y(a=value1)) - E(Y(a=value2)). Enter a=value1 for average counterfactual outcome estimation at the specified treatment level value1.")

    }else if (is.vector(a) & length(a)==2){ ## ATE estimate ==

      ## TMLE estimator

      out.a1 <- backdoor.TMLE.a(a = a[1], data = data, vertices = vertices, di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome, multivariate.variables = multivariate.variables, graph = graph,

                                superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                                superlearner.A = superlearner.A, # whether run superlearner for propensity score

                                crossfit = crossfit, K = K,

                                lib.Y = lib.Y, # superlearner library for outcome regression
                                lib.A = lib.A, # superlearner library for propensity score

                                formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                                linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                                truncate_lower = truncate_lower, truncate_upper = truncate_upper)

      out.a0 <- backdoor.TMLE.a(a = a[2], data = data, vertices = vertices, di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome, multivariate.variables = multivariate.variables, graph = graph,

                                superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                                superlearner.A = superlearner.A, # whether run superlearner for propensity score

                                crossfit = crossfit, K = K,

                                lib.Y = lib.Y, # superlearner library for outcome regression
                                lib.A = lib.A, # superlearner library for propensity score

                                formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                                linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                                truncate_lower = truncate_lower, truncate_upper = truncate_upper)

      ############################ aipw ############################
      # run aipw
      aipw_output_Y1 <- out.a1$aipw
      aipw_output_Y0 <- out.a0$aipw

      # estimate E[Y(1)], E[Y(0)], and ATE
      hat_E.Y1 = aipw_output_Y1$estimated_psi
      hat_E.Y0 = aipw_output_Y0$estimated_psi
      hat_ATE = hat_E.Y1 - hat_E.Y0

      # lower CI
      lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((aipw_output_Y1$EIF-aipw_output_Y0$EIF)^2)/n)

      # upper CI
      upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((aipw_output_Y1$EIF-aipw_output_Y0$EIF)^2)/n)

      aipw.out <- list(ATE=hat_ATE, # estimated parameter
                       lower.ci=lower.ci_ATE, # lower bound of 95% CI
                       upper.ci=upper.ci_ATE, # upper bound of 95% CI
                       EIF=aipw_output_Y1$EIF-aipw_output_Y0$EIF # EIF
      )


      ############################ gcomp ############################
      # run gcomp
      gcomp_output_Y1 <- out.a1$gcomp
      gcomp_output_Y0 <- out.a0$gcomp

      # estimate E[Y(1)], E[Y(0)], and ATE
      hat_E.Y1 = gcomp_output_Y1$estimated_psi
      hat_E.Y0 = gcomp_output_Y0$estimated_psi
      hat_ATE = hat_E.Y1 - hat_E.Y0

      # lower CI
      lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((gcomp_output_Y1$EIF-gcomp_output_Y0$EIF)^2)/n)

      # upper CI
      upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((gcomp_output_Y1$EIF-gcomp_output_Y0$EIF)^2)/n)

      gcomp.out <- list(ATE=hat_ATE, # estimated parameter
                        lower.ci=lower.ci_ATE, # lower bound of 95% CI
                        upper.ci=upper.ci_ATE, # upper bound of 95% CI
                        EIF=gcomp_output_Y1$EIF-gcomp_output_Y0$EIF # EIF
      )

      ############################ ipw ############################
      # run ipw
      ipw_output_Y1 <- out.a1$ipw
      ipw_output_Y0 <- out.a0$ipw

      # estimate E[Y(1)], E[Y(0)], and ATE
      hat_E.Y1 = ipw_output_Y1$estimated_psi
      hat_E.Y0 = ipw_output_Y0$estimated_psi
      hat_ATE = hat_E.Y1 - hat_E.Y0

      # lower CI
      lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((ipw_output_Y1$EIF-ipw_output_Y0$EIF)^2)/n)

      # upper CI
      upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((ipw_output_Y1$EIF-ipw_output_Y0$EIF)^2)/n)

      ipw.out <- list(ATE=hat_ATE, # estimated parameter
                      lower.ci=lower.ci_ATE, # lower bound of 95% CI
                      upper.ci=upper.ci_ATE, # upper bound of 95% CI
                      EIF=ipw_output_Y1$EIF-ipw_output_Y0$EIF # EIF
      )

      cat(paste0("AIPW estimated ACE: ",round(aipw.out$ATE,2),"; 95% CI: (",round(aipw.out$lower.ci,2),", ",round(aipw.out$upper.ci,2),") \n",
                 "IPW estimated ACE: ",round(ipw.out$ATE,2),"; 95% CI: (",round(ipw.out$lower.ci,2),", ",round(ipw.out$upper.ci,2),") \n",
                 "G-comp estimated ACE: ",round(gcomp.out$ATE,2),"; 95% CI: (",round(gcomp.out$lower.ci,2),", ",round(gcomp.out$upper.ci,2),")"))

      np.out <- list(AIPW=aipw.out, IPW=ipw.out, Gcomp=gcomp.out,
                     AIPW_Y1 = aipw_output_Y1, AIPW_Y0 = aipw_output_Y0, Gcomp_Y1 = gcomp_output_Y1,
                     Gcomp_Y0 = gcomp_output_Y0, IPW_Y1 = ipw_output_Y1, IPW_Y0 = ipw_output_Y0)


    }else if (length(a)==1) { ## E(Y^1) estimate ==

      out.a <- NPS.TMLE.a(a = a, data = data, vertices = vertices,
                          di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome,
                          multivariate.variables = multivariate.variables, graph = graph,

                          superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                          superlearner.A = superlearner.A, # whether run superlearner for propensity score

                          crossfit = crossfit, K = K,

                          lib.Y = lib.Y, # superlearner library for outcome regression
                          lib.A = lib.A, # superlearner library for propensity score

                          formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                          linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                          truncate_lower = truncate_lower, truncate_upper = truncate_upper)

      np.out <- out.a

    } # end of if else condition for testing the length of a






  }else if (!is.p.fix(graph, treatment)){ # the graph is not fixable: if the graph is primal fixable




    stop("The treatment effect may or may not be identified. Further investigation is needed.") # the graph is not primal fixable



  }else{ # the graph is not fixable: the graph is primal fixable

    ############################################################
    # Perform estimation with the NPS method
    ############################################################

    if (is.vector(a) & length(a)>2){ ## Invalid input ==

      print("Invalid input. Enter a=c(vaule1,value2) for average causal effect estimation: (Y(a=value1)) - E(Y(a=value2)). Enter a=value1 for average counterfactual outcome estimation at the specified treatment level value1.")

    }else if (is.vector(a) & length(a)==2){ ## ATE estimate ==

      ## TMLE estimator

      out.a1 <- NPS.TMLE.a(a = a[1], data = data, vertices = vertices, di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome, multivariate.variables = multivariate.variables, graph = graph,

                           superlearner.seq = superlearner.seq, # whether run superlearner for sequential regression
                           superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                           superlearner.A = superlearner.A, # whether run superlearner for propensity score
                           superlearner.M = superlearner.M, # whether run superlearner for estimating densratio for M using bayes method
                           superlearner.L = superlearner.L, # whether run superlearner for estimating densratio for L using bayes method

                           crossfit = crossfit, K = K,

                           ratio.method.L = ratio.method.L, # method for estimating the density ratio associated with M
                           ratio.method.M = ratio.method.M, # method for estimating the density ratio associated with L

                           lib.seq = lib.seq, # superlearner library for sequential regression
                           lib.L = lib.L, # superlearner library for density ratio estimation via bayes rule for variables in L
                           lib.M = lib.M, # superlearner library for density ratio estimation via bayes rule for variables in M
                           lib.Y = lib.Y, # superlearner library for outcome regression
                           lib.A = lib.A, # superlearner library for propensity score

                           formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                           linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                           n.iter = n.iter, cvg.criteria = cvg.criteria,
                           truncate_lower = truncate_lower, truncate_upper = truncate_upper,zerodiv.avoid=zerodiv.avoid)

      out.a0 <- NPS.TMLE.a(a = a[2], data = data, vertices = vertices, di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome, multivariate.variables = multivariate.variables, graph = graph,

                           superlearner.seq = superlearner.seq, # whether run superlearner for sequential regression
                           superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                           superlearner.A = superlearner.A, # whether run superlearner for propensity score
                           superlearner.M = superlearner.M, # whether run superlearner for estimating densratio for M using bayes method
                           superlearner.L = superlearner.L, # whether run superlearner for estimating densratio for L using bayes method

                           crossfit = crossfit, K = K,

                           ratio.method.L = ratio.method.L, # method for estimating the density ratio associated with M
                           ratio.method.M = ratio.method.M, # method for estimating the density ratio associated with L

                           lib.seq = lib.seq, # superlearner library for sequential regression
                           lib.L = lib.L, # superlearner library for density ratio estimation via bayes rule for variables in L
                           lib.M = lib.M, # superlearner library for density ratio estimation via bayes rule for variables in M
                           lib.Y = lib.Y, # superlearner library for outcome regression
                           lib.A = lib.A, # superlearner library for propensity score

                           formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                           linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                           n.iter = n.iter, cvg.criteria = cvg.criteria,
                           truncate_lower = truncate_lower, truncate_upper = truncate_upper,zerodiv.avoid=zerodiv.avoid)

      ############################ TMLE ############################
      # run TMLE
      tmle_output_Y1 <- out.a1$TMLE
      tmle_output_Y0 <- out.a0$TMLE

      # estimate E[Y(1)], E[Y(0)], and ATE
      hat_E.Y1 = tmle_output_Y1$estimated_psi
      hat_E.Y0 = tmle_output_Y0$estimated_psi
      hat_ATE = hat_E.Y1 - hat_E.Y0

      # lower CI
      lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)

      # upper CI
      upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((tmle_output_Y1$EIF-tmle_output_Y0$EIF)^2)/n)

      tmle.out <- list(ATE=hat_ATE, # estimated parameter
                       lower.ci=lower.ci_ATE, # lower bound of 95% CI
                       upper.ci=upper.ci_ATE, # upper bound of 95% CI
                       EIF=tmle_output_Y1$EIF-tmle_output_Y0$EIF # EIF
      )


      ############################ onestep ############################
      # run onestep
      onestep_output_Y1 <- out.a1$Onestep
      onestep_output_Y0 <- out.a0$Onestep

      # estimate E[Y(1)], E[Y(0)], and ATE
      hat_E.Y1 = onestep_output_Y1$estimated_psi
      hat_E.Y0 = onestep_output_Y0$estimated_psi
      hat_ATE = hat_E.Y1 - hat_E.Y0

      # lower CI
      lower.ci_ATE = hat_ATE - 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)

      # upper CI
      upper.ci_ATE = hat_ATE + 1.96*sqrt(mean((onestep_output_Y1$EIF-onestep_output_Y0$EIF)^2)/n)

      onestep.out <- list(ATE=hat_ATE, # estimated parameter
                          lower.ci=lower.ci_ATE, # lower bound of 95% CI
                          upper.ci=upper.ci_ATE, # upper bound of 95% CI
                          EIF=onestep_output_Y1$EIF-onestep_output_Y0$EIF # EIF
      )

      cat(paste0("TMLE estimated ACE: ",round(tmle.out$ATE,2),"; 95% CI: (",round(tmle.out$lower.ci,2),", ",round(tmle.out$upper.ci,2),") \n","Onestep estimated ACE: ",round(onestep.out$ATE,2),"; 95% CI: (",round(onestep.out$lower.ci,2),", ",round(onestep.out$upper.ci,2),")"))

      np.out <- list(TMLE=tmle.out,Onestep=onestep.out, TMLE.Y1=tmle_output_Y1, TMLE.Y0 = tmle_output_Y0, Onestep.Y1=onestep_output_Y1, Onestep.Y0=onestep_output_Y0)


    }else if (length(a)==1) { ## E(Y^1) estimate ==

      out.a <- NPS.TMLE.a(a = a, data = data, vertices = vertices,
                          di_edges = di_edges, bi_edges = bi_edges, treatment = treatment, outcome = outcome,
                          multivariate.variables = multivariate.variables, graph = graph,

                          superlearner.seq = superlearner.seq, # whether run superlearner for sequential regression
                          superlearner.Y = superlearner.Y, # whether run superlearner for outcome regression
                          superlearner.A = superlearner.A, # whether run superlearner for propensity score
                          superlearner.M = superlearner.M, # whether run superlearner for estimating densratio for M using bayes method
                          superlearner.L = superlearner.L, # whether run superlearner for estimating densratio for L using bayes method

                          crossfit = crossfit, K = K,

                          ratio.method.L = ratio.method.L, # method for estimating the density ratio associated with M
                          ratio.method.M = ratio.method.M, # method for estimating the density ratio associated with L

                          lib.seq = lib.seq, # superlearner library for sequential regression
                          lib.L = lib.L, # superlearner library for density ratio estimation via bayes rule for variables in L
                          lib.M = lib.M, # superlearner library for density ratio estimation via bayes rule for variables in M
                          lib.Y = lib.Y, # superlearner library for outcome regression
                          lib.A = lib.A, # superlearner library for propensity score

                          formulaY = formulaY, formulaA = formulaA, # regression formula for outcome regression and propensity score if superlearner is not used
                          linkY_binary = linkY_binary, linkA = linkA, # link function for outcome regression and propensity score if superlearner is not used

                          n.iter = n.iter, cvg.criteria = cvg.criteria,
                          truncate_lower = truncate_lower, truncate_upper = truncate_upper, zerodiv.avoid=zerodiv.avoid)
      np.out <- out.a

    } # end of if else condition for testing the length of a



  } # end of if else for fix, p-fix







  #####################################################
  # NPS and semi-parametric model
  #####################################################

  if (is.np.saturated(graph)){

    print("The graph is nonparametrically saturated. The nonparametric TMLE and one-step estimator results are provided, which are in theory the most efficient estimator.")

    return(np.out)

  }else if (is.mb.shielded(graph)){


    #### ADD code for semi-parametric one step estimation ####


    print("The graph is mb-shield. The semi.out contains the semi-parametric one-step estimator result, which in theory is the most efficient estimator. The np.out contains the non-parametric TMLE and one-step estimator results.")

    return(list(np.out=np.out, semi.out=semi.out))

  }else{

    print("Estimation provide via imposing NO independence constraints among variables. Note that there may be more efficient estimators.")

    return(np.out)

  }











} # end of ADMGtmle function


