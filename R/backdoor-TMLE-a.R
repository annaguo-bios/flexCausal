## Provide G-comp, Inverse Probability Weighting (IPW), and Augmented IPW (AIPW) estimators ====
#' Estimate average counterfactual outcome E(Y(a)).
#'
#' Function for estimating the average counterfactual outcome E(Y(a)) via backdoor adjustment. The adjustment set is the Markov pillow of the treatment variable A.
#' @param a Treatment level or a length two vector specifying treatment levels.
#'          The average counterfactual outcome(s) will be computed at the specified treatment level(s).
#'          If `a` is a single value, the function will return `E(Y(a))`, which is the
#'          average counterfactual outcome under the treatment level specified by `a`.
#'          If `a` is a vector of length two, say `c(value1, value2)`, the function will return
#'          `E(Y(a=value1)) - E(Y(a=value2))`, which is the contrast between the average
#'          counterfactual outcomes under the two specified treatment levels.
#' @param data A Dataframe contains all the variables listed in vertices parameter
#' @param vertices A vector of variable names in the causal graph
#' @param di_edges A list of directed edges in the causal graph. For example, `di_edges=list(c('A','B'))` means there is a directed edge from vertex A to B.
#' @param bi_edges A list of bidirected edges in the causal graph. For example, `bi_edges=list(c('A','B'))` means there is a bidirected edge between vertex A and B.
#' @param multivariate.variables A list of variables that are multivariate in the causal graph. For example, `multivariate.variables=list(M=c('M1,'M2'))` means M is bivariate and the corresponding columns in the dataframe are M1 and M2.
#' @param graph A graph object generated by the `make.graph()` function. User only need to specify either `graph` or `vertices`, `di_edges`, `bi_edges`, and `multivariate.variables`.
#' @param treatment A character string indicating the treatment variable
#' @param outcome A character string indicating the outcome variable
#' @param superlearner.Y A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the outcome regression.
#' @param superlearner.A A logical indicator determines whether SuperLearner via the \link[SuperLearner]{SuperLearner} function is adopted for estimating the propensity score.
#' @param crossfit A logical indicator determines whether crossfitting is adopted for SuperLearner. If crossfit is set to TRUE, the data is split into K folds as specified by the `K` parameter.
#' @param K An integer specifying the number of folds for crossfitting.
#' @param lib.Y A character vector specifying the library of algorithms to be used in the SuperLearner for outcome regression.
#' @param lib.A A character vector specifying the library of algorithms to be used in the SuperLearner for propensity score estimation.
#' @param formulaY Regression formula for the outcome regression of Y on it's Markov pillow. The default is 'Y ~ .'.
#' @param formulaA Regression formula for the propensity score regression of A on it's Markov pillow. The default is 'A ~ .'.
#' @param linkY_binary The link function used for outcome regression of Y on it's Markov pillow when Y is binary and superlearner is not sued. The default is the 'logit' link.
#' @param linkA The link function used for propensity score regression of A on it's Markov pillow if superlearner is not used. The default is the 'logit' link.
#' @param truncate_lower The lower bound for truncation of the propensity score. The default is 0, which means no truncation.
#' @param truncate_upper The upper bound for truncation of the propensity score. The default is 1, which means no truncation.
#' @return Function outputs a list containing TMLE results and onestep results:
#' \describe{
#'       \item{\code{estimated_psi}}{The estimated parameter of interest: \eqn{E(Y^a)}}
#'       \item{\code{lower.ci}}{Lower bound of the 95% confidence interval for \code{estimated_psi}}
#'       \item{\code{upper.ci}}{Upper bound of the 95 pct confidence interval for \code{estimated_psi}}
#'       \item{\code{EIF}}{The estimated efficient influence function evaluated at the observed data}
#'       \item{\code{p.a.mpA}}{The estimated propensity score at treatment level \eqn{A=a} given the Markov pillow of A}
#'       \item{\code{mu.Y_a0}}{Estimated \eqn{E[Y|a,mp(A)]} for v that comes right after A in topological order}}
#' @examples
#' # E(Y(1)) estimation.
#' backdoor.TMLE.a(a=1,data=data_backdoor, vertices=c('A','Y','X'),
#' di_edges=list(c('X','A'), c('X','Y'), c('A','Y')),
#' treatment='A', outcome='Y')
#' @importFrom dplyr %>% mutate select
#' @importFrom MASS mvrnorm
#' @importFrom SuperLearner CV.SuperLearner SuperLearner
#' @importFrom mvtnorm dmvnorm
#' @importFrom densratio densratio
#' @importFrom utils combn
#' @importFrom stats rnorm runif rbinom dnorm dbinom binomial gaussian predict glm as.formula qlogis plogis lm coef cov sd
#' @export
#'
#'
#'
#'
backdoor.TMLE.a <- function(a=NULL,data=NULL,vertices=NULL, di_edges=NULL, bi_edges=NULL, treatment=NULL, outcome=NULL, multivariate.variables=NULL, graph=NULL,
                       superlearner.Y=F, # whether run superlearner for outcome regression
                       superlearner.A=F, # whether run superlearner for propensity score
                       crossfit=F, K=5,
                       lib.Y = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for outcome regression
                       lib.A = c("SL.glm","SL.earth","SL.ranger","SL.mean"), # superlearner library for propensity score
                       formulaY="Y ~ .", formulaA="A ~ .", # regression formula for outcome regression and propensity score if superlearner is not used
                       linkY_binary="logit", linkA="logit", # link function for outcome regression and propensity score if superlearner is not used
                       truncate_lower=0, truncate_upper=1){

  n <- nrow(data)

  a0 <- a
  a1 <- 1-a

  # extract graph components
  if (!is.null(graph)){

    vertices <- graph$vertices
    di_edges <- graph$di_edges
    bi_edges <- graph$bi_edges
    multivariate.variables <- graph$multivariate.variables

  }else{

    # make a graph object
    graph <- make.graph(vertices=vertices, bi_edges=bi_edges, di_edges=di_edges, multivariate.variables=multivariate.variables)

  }

  # Variables
  A <- data[,treatment] # treatment
  Y <- data[,outcome] # outcome



  ################################################
  ############### OUTCOME REGRESSION #############
  ################################################

  #### Fit nuisance models ####

  # Find Markov pillow of the treatment
  mpA <- f.markov_pillow(graph, treatment, treatment) # Markov pillow for A
  mpA <- replace.vector(mpA, multivariate.variables) # replace vertices with it's components if vertices are multivariate

  # prepare dataset for regression and prediction
  dat_Y <- data[,c(mpA,treatment), drop = F] # extract data for Markov pillow for outcome
  dat_Y.a0 <- dat_Y %>% mutate(!!treatment := a0) # set treatment to a0
  dat_Y.a1 <- dat_Y %>% mutate(!!treatment := a1) # set treatment to a1

  if (crossfit==T){ #### cross fitting + super learner #####

    fit.family <- if(all(Y %in% c(0,1))){binomial()}else{gaussian()} # family for super learner depending on whether Y is binary or continuous

    or_fit <- CV.SuperLearner(Y=Y, X=dat_Y, family = fit.family, V = K, SL.library = lib.Y, control = list(saveFitLibrary=T),saveAll = T)

    mu.Y_a1 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=dat_Y.a1[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
    mu.Y_a0 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=dat_Y.a0[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]


  } else if (superlearner.Y==T){ #### super learner #####

    fit.family <- if(all(Y %in% c(0,1))){binomial()}else{gaussian()} # family for super learner depending on whether Y is binary or continuous

    or_fit <- SuperLearner(Y=Y, X=dat_Y, family = fit.family, SL.library = lib.Y)

    mu.Y_a1 <- predict(or_fit, newdata=dat_Y.a1)[[1]] %>% as.vector()
    mu.Y_a0 <- predict(or_fit, newdata=dat_Y.a0)[[1]] %>% as.vector()

  } else { #### simple linear regression with user input regression formula: default="Y ~ ." ####

    fit.family <- if(all(Y %in% c(0,1))){binomial()}else{gaussian()} # family for super learner depending on whether Y is binary or continuous

    or_fit <- glm(as.formula(formulaY), data=dat_Y, family = fit.family)

    mu.Y_a1 <- predict(or_fit, newdata=dat_Y.a1)
    mu.Y_a0 <- predict(or_fit, newdata=dat_Y.a0)


  }


  ################################################
  ############### PROPENSITY SCORE ###############
  ################################################

  #### Fit nuisance models ####

  # if mpA is an empty set, then we just get the propensity score as the mean of the treatment
  if (length(mpA)==0){

    p.A1.mpA <- rep(mean(A==1), nrow(data))

  }else{

    # prepare dataset for regression and prediction
    dat_mpA <- data[,mpA, drop = F] # extract data for Markov pillow for outcome

    if (crossfit==T){ #### cross fitting + super learner #####

      # fit model
      ps_fit <- CV.SuperLearner(Y=A, X=dat_mpA, family = binomial(), V = K, SL.library = lib.A, control = list(saveFitLibrary=T),saveAll = T)

      # make prediction: p(A=1|mp(A))
      p.A1.mpA <- ps_fit$SL.predict

    } else if (superlearner.A==T){ #### super learner #####

      # fit model
      ps_fit <- SuperLearner(Y=A, X=dat_mpA, family = binomial(), SL.library = lib.A)

      # make prediction: p(A=1|mp(A))
      p.A1.mpA <- predict(ps_fit, type = "response")[[1]] %>% as.vector()

    } else { # without weak overlapping issue. Run A~X via logistic regression

      # fit model
      ps_fit <- glm(as.formula(formulaA), data=dat_mpA,  family = binomial())

      # make prediction: p(A=1|mp(A))
      p.A1.mpA <- predict(ps_fit, type = "response")  # p(A=1|X)

    }
  }

  # apply truncation to propensity score to deal with weak overlap.
  # truncated propensity score within the user specified range of [truncate_lower, truncate_upper]: default=[0,1]
  p.a0.mpA <- a0*p.A1.mpA + (1-a0)*(1-p.A1.mpA) # p(A=a1|mp(A))

  p.a0.mpA[p.a0.mpA < truncate_lower] <- truncate_lower
  p.a0.mpA[p.a0.mpA > truncate_upper] <- truncate_upper


  ## AIPW
  aipw <- mean((A==a0)*(Y-mu.Y_a0)/p.a0.mpA+mu.Y_a0)

  ## EIF
  EIF <- (A==a0)*(Y-mu.Y_a0)/p.a0.mpA+mu.Y_a0 - aipw


  # confidence interval
  lower.ci <- aipw-1.96*sqrt(mean(EIF^2)/nrow(data))
  upper.ci <- aipw+1.96*sqrt(mean(EIF^2)/nrow(data))

  aipw <- list(estimated_psi=aipw, # estimated parameter
               lower.ci=lower.ci, # lower bound of 95% CI
               upper.ci=upper.ci, # upper bound of 95% CI
               EIF=EIF, # E(Dstar) for Y|M,A,X and M|A,X, and A|X
               p.a.mpA = p.a0.mpA, # estimated E[A=a1|mp(A)]
               mu.Y_a = mu.Y_a0 # estimated E[Y|A=a0,mp(A)]
  )

  ## G-comp
  gcomp <- mean(mu.Y_a0)

  # confidence interval
  lower.ci <- gcomp-1.96*sqrt(mean(EIF^2)/nrow(data))
  upper.ci <- gcomp+1.96*sqrt(mean(EIF^2)/nrow(data))

  gcomp <- list(estimated_psi=gcomp, # estimated parameter
               lower.ci=lower.ci, # lower bound of 95% CI
               upper.ci=upper.ci, # upper bound of 95% CI
               EIF=EIF, # E(Dstar) for Y|M,A,X and M|A,X, and A|X
               mu.Y_a = mu.Y_a0 # estimated E[Y|A=a0,mp(A)]
  )

  ## IPW
  ipw <- mean((A==a0)*Y/p.a0.mpA)

  # confidence interval
  lower.ci <- ipw-1.96*sqrt(mean(EIF^2)/nrow(data))
  upper.ci <- ipw+1.96*sqrt(mean(EIF^2)/nrow(data))

  ipw <- list(estimated_psi=ipw, # estimated parameter
                lower.ci=lower.ci, # lower bound of 95% CI
                upper.ci=upper.ci, # upper bound of 95% CI
                EIF=EIF, # E(Dstar) for Y|M,A,X and M|A,X, and A|X
                p.a.mpA = p.a0.mpA # estimated E[A=a1|mp(A)]
  )

  return(list(aipw=aipw, gcomp=gcomp, ipw=ipw))








}
