bayes_fit <- .force_weight(bayes_fit, model="cv", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
}else if (superlearner.M==T){
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.M)
bayes_fit <- .force_weight(bayes_fit, model="sl", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
} else {
# estimate density ratio using bayes rule
bayes_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
}
}
ratio <- ratio.num/ratio.den # p(M|mp(M))|_{a_0}/p(M|mp(M))|_{a_1}
assign(paste0("densratio_",v), ratio)
}
} else if (ratio.method.M=="dnorm"){ ################### METHOD 2B: dnorm method  ###################
if (!all(sapply(replace.vector(M.mpM.includeA, multivariate.variables), function(var) is.numeric(data[,var]) | is.integer(data[,var]) | length(unique(data[,var]))==2 ))){
print("Error in estimating density ratios associated with variables in M: dnorm method only support continuous or binary variables, try bayes method instead.")
stop() }
# if M consists of continuous or binary variables: apply density ratio estimation via dnorm
for (v in M.mpM.includeA){ ## Iterate over each variable in L\A
ratio <- calculate_density_ratio_dnorm(a0=a0, v , graph, treatment=treatment, data=data) # p(M|mp(M))|_{a_0}/p(M|mp(M))|_{a_1}
assign(paste0("densratio_",v), ratio)
}
}else if (ratio.method.M=="bayes"){ ################### METHOD 2C: Bayes method ###################
M.use.densratioA <- c()
for (v in M.mpM.includeA){ ## Iterate over each variable in M
#### Prepare data for regression and prediction ####
dat_bayes.v <- data[,setdiff(replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables) ,treatment), drop=F] # contains variable v + Markov pillow of v - treatment
names(dat_bayes.v)[names(dat_bayes.v)=="Y"] <- "outcome" # Super Learner get confused of the regressor contains a variable named Y. Thus rename it to outcome
dat_bayes.v_v <- data[,setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables) ,treatment), drop=F] # contains variable Markov pillow of v - treatment
#### Fit nuisance models ####
if (crossfit==T){
# fit p(A|mp(v)\A,v)
bayes_fit <- CV.SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), V = K, SL.library = lib.M, control = list(saveFitLibrary=T),saveAll = T)
bayes_fit <- .force_weight(bayes_fit, model="cv", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(A=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(A=a1|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
if (are_same(setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables) ,treatment), replace.vector(f.markov_pillow(graph,treatment, treatment), multivariate.variables))){
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
M.use.densratioA <- c(M.use.densratioA, v)
}else{
bayes_fit_v <- CV.SuperLearner(Y=A, X=dat_bayes.v_v, family = binomial(), V = K, SL.library = lib.M, control = list(saveFitLibrary=T),saveAll = T)
bayes_fit_v <- .force_weight(bayes_fit_v, model="cv", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A)
p.A1.mpv_v <- bayes_fit_v$SL.predict  # p(A=1|X)
#p(v=a0|mp(v)\A)
p.a0.mpv_v <- a0*p.A1.mpv_v+(1-a0)*(1-p.A1.mpv_v)
#p(v=a0|mp(v)\A)
p.a1.mpv_v <- 1-p.a0.mpv_v
p.a1.mpv_v[p.a1.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv_v[p.a0.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/{p.a0.mpv_v/p.a1.mpv_v})
}
}else if (superlearner.M==T){
# fit p(A|mp(v)\A,v)
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.M)
bayes_fit <- .force_weight(bayes_fit, model="sl", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(A=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(A=a1|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
if (are_same(setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables) ,treatment), replace.vector(f.markov_pillow(graph,treatment, treatment), multivariate.variables))){
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
M.use.densratioA <- c(M.use.densratioA, v)
}else{
bayes_fit_v <- SuperLearner(Y=A, X=dat_bayes.v_v, family = binomial(), SL.library = lib.M)
bayes_fit_v <- .force_weight(bayes_fit_v, model="sl", lib=lib.M) # force weight to be 1 for the one algorithm
# p(A=1|mp(v)\A)
p.A1.mpv_v <- predict(bayes_fit_v, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(v=a0|mp(v)\A)
p.a0.mpv_v <- a0*p.A1.mpv_v+(1-a0)*(1-p.A1.mpv_v)
#p(v=a0|mp(v)\A)
p.a1.mpv_v <- 1-p.a0.mpv_v
p.a1.mpv_v[p.a1.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv_v[p.a0.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/{p.a0.mpv_v/p.a1.mpv_v})
}
} else {
# estimate density ratio using bayes rule
bayes_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
if (are_same(setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables) ,treatment), replace.vector(f.markov_pillow(graph,treatment, treatment), multivariate.variables))){
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
M.use.densratioA <- c(M.use.densratioA, v)
}else{
bayes_fit_v <- glm(A ~ ., data=dat_bayes.v_v, family = binomial())
# p(A=1|mp(v)\A)
p.A1.mpv_v <- predict(bayes_fit_v, type = "response")
#p(v=a0|mp(v)\A)
p.a0.mpv_v <- a0*p.A1.mpv_v+(1-a0)*(1-p.A1.mpv_v)
#p(v=a0|mp(v)\A)
p.a1.mpv_v <- 1-p.a0.mpv_v
p.a1.mpv_v[p.a1.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv_v[p.a0.mpv_v<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/{p.a0.mpv_v/p.a1.mpv_v})
}
}
} ## Iterate over each variable in M
} else {
print("Invalid ratio.method.M input.")
stop()
}
# Get the densratio vectors based on their names
densratio.vectors.M <- mget(paste0("densratio_",M))
# Create a data frame using all the vectors
densratio.M <- data.frame(densratio.vectors.M)
# the sequential regression will be perform for v that locates between A and Y accroding to topological order tau
vertices.between.AY <- tau[{which(tau==treatment)+1}:{which(tau==outcome)-1}]
# sequential regression
for (v in rev(vertices.between.AY)){ ## iterate through vertices between A and Y according to topological order tau
# perform sequential regressions for v with larger order first: rev()
### Prepare Markov pillow and dataset for regression and prediction ####
# Find Markov pillow of v
mpv <- f.markov_pillow(graph, v, treatment) # Markov pillow for outcome
mpv <- replace.vector(mpv, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpv <- data[,mpv] # extract data for Markov pillow for v
if (treatment %in% mpv){ # we only need to consider evaluate regression at specific level of A if treatment is in Markov pillow for v
# set treatment to a0
dat_mpv.a0 <- dat_mpv %>% mutate(!!treatment := a0)
# set treatment to a1
dat_mpv.a1 <- dat_mpv %>% mutate(!!treatment := a1) }
# vertex that right after Z according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1")) }else{ get(paste0("mu.",next.v,"_a0")) } # mu is evaluated at a0 for next.v in M and at a1 for next.v in L
next.mu.transform <- if(all(Y %in% c(0,1))){qlogis(next.mu)}else{next.mu} # if Y is binary, logit transform the sequential regression first
### End of preparation ####
if (crossfit==T){ #### cross fitting + super learner #####
v_fit <- CV.SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), V = K, SL.library = lib.seq, control = list(saveFitLibrary=T), saveAll = T)
v_fit <- .force_weight(v_fit, model="cv", lib=lib.seq) # force weight to be 1 for the one algorithm
######## prediction: A in mp(v) vs A NOT in mp(v) ########
if (treatment %in% mpv){
reg_a1 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a1[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
reg_a0 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a0[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
}
#########################################################
} else if (superlearner.seq==T){ #### super learner #####
v_fit <- SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), SL.library = lib.seq)
v_fit <- .force_weight(v_fit, model="sl", lib=lib.seq) # force weight to be 1 for the one algorithm
######## prediction: A in mp(v) vs A NOT in mp(v) ########
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)[[1]] %>% as.vector()
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)[[1]] %>% as.vector()
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary)
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
}
#########################################################
}else { #### linear regression #####
v_fit <- lm(next.mu.transform ~ ., data=dat_mpv) # fit linear regression/ logistic regression depending on type of v
######## prediction: A in mp(v) vs A NOT in mp(v)########
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
}
##########################################################
}
}  ## End of iteration over all vertics between A and Y
# density ratio before Y
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==outcome])] # M precedes Y
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==outcome])] # L precedes Y
f.M_preY <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede Y = the set M
f.L_preY <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede Y = the set L
EIF.Y <- if(outcome %in% L){
(A==a1)*f.M_preY*(Y-mu.Y_a1)}else{ # if Y in L
(A==a0)*1/f.L_preY*(Y-mu.Y_a0)} # if Y in M
for (v in rev(vertices.between.AY)){ ## iterate over all vertices between A and Y
# select M and L that precede v
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==v])] # M precedes Z
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==v])] # L precedes Z
# vertex that right after v according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
# mu(next.v, a_{next.v})
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1"))}else{get(paste0("mu.",next.v,"_a0"))}
EIF.v <- if(v %in% L){ # v in L
# product of the selected variables density ratio
f.M_prev <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede v
# EIF for v|mp(v)
(A==a1)*f.M_prev*( next.mu - get(paste0("mu.",v,"_a1")) )
}else{ # v in M
# product of the selected variables density ratio
f.L_prev <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede v
# EIF for v|mp(v)
(A==a0)*1/f.L_prev*( next.mu - get(paste0("mu.",v,"_a0")) )
}
assign(paste0("EIF.",v), EIF.v)
} ## End of iteration over all vertices between A and Y
# vertex that right after A according to tau
next.A <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==treatment]+1}]
EIF.A <- {(A==a1) - p.a1.mpA}*get(paste0("mu.",next.A,"_a0"))
# estimated psi
estimated_psi = mean( EIF.Y + rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))) +  EIF.A + p.a1.mpA*get(paste0("mu.",next.A,"_a0")) + (A==a0)*Y )
# EIF
EIF <- EIF.Y + # EIF of Y|mp(Y)
rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))) + # EIF of v|mp(v) for v between A and Y
EIF.A + # EIF of A|mp(A)
p.a1.mpA*get(paste0("mu.",next.A,"_a0")) + # EIF of mp(A)
mean((A==a0)*Y) -
estimated_psi
# confidence interval
lower.ci <- estimated_psi-1.96*sqrt(mean(EIF^2)/nrow(data))
upper.ci <- estimated_psi+1.96*sqrt(mean(EIF^2)/nrow(data))
onestep.out <- list(estimated_psi=estimated_psi, # estimated parameter
lower.ci=lower.ci, # lower bound of 95% CI
upper.ci=upper.ci, # upper bound of 95% CI
EIF=EIF, # E(Dstar) for Y|M,A,X and M|A,X, and A|X
EIF.Y=EIF.Y, # EIF of Y|mp(Y)
EIF.A=EIF.A, # EIF of A|mp(A)
EIF.v = rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))), # EIF of v|mp(v) for v between A and Y
p.a1.mpA = p.a1.mpA, # estimated E[A=a1|mp(A)]
mu.next.A = get(paste0("mu.",next.A,"_a0")) # estimated E[v|mp(v)] for v that comes right after A
)
# initialize EDstar: the mean of the EIF for A, Y, and v
EDstar <- 10 # random large number
# record EDstar over iterations
EDstar.record <- c()
# initialize iteration counter
iter <- 0
while(abs(EDstar) > cvg.criteria & iter < n.iter){
######################
# update p(A=a1|mp(A))
######################
# clever coefficient for propensity score: mu(Z_1,a0)
clevercoef.A <- get(paste0("mu.",next.A,"_a0"))
# derive epsA
ind <- A==a1
ps_model <- glm(
ind ~ offset(qlogis(p.a1.mpA))+ clevercoef.A -1, family=binomial(), start=0
)
eps.A <- coef(ps_model)
# update p(A=a1|mp(A))
p.a1.mpA <- plogis(qlogis(p.a1.mpA)+eps.A*(clevercoef.A))
p.a0.mpA <-1-p.a1.mpA # p(A=a0|mp(A))
# avoid zero indivision error
p.a0.mpA[p.a0.mpA<zerodiv.avoid] <- zerodiv.avoid
p.a1.mpA[p.a1.mpA<zerodiv.avoid] <- zerodiv.avoid
# update density ratio of A
assign("densratio_A", p.a0.mpA/p.a1.mpA) # density ratio regarding the treatment p(A|mp(A))|_{a_0}/p(A|mp(A))|_{a_1}
# update density ratio for v in L if the ratio.method.L = "bayes" OR "densratio"
if (ratio.method.L == "bayes"){ for (v in L.use.densratioA){assign(paste0("densratio_",v), get(paste0("bayes.densratio_",v))/densratio_A)} }
if (ratio.method.L == "densratio"){ for (v in L.use.densratioA){assign(paste0("densratio_",v), 1/(get(paste0("densratio.densratio_",v))*densratio_A))} }
# Update the density ratio data frame for L
densratio.vectors.L <- mget(paste0("densratio_",L))
densratio.L <- data.frame(densratio.vectors.L)
# update density ratio for v in M if the ratio.method.M = "bayes" OR "densratio"
if (ratio.method.M == "bayes"){ for (v in M.use.densratioA){assign(paste0("densratio_",v), get(paste0("bayes.densratio_",v))/densratio_A)} }
if (ratio.method.M == "densratio"){ for (v in M.use.densratioA){assign(paste0("densratio_",v), get(paste0("densratio.densratio_",v))/densratio_A) } }
# Update the density ratio data frame for M
densratio.vectors.M <- mget(paste0("densratio_",M))
densratio.M <- data.frame(densratio.vectors.M)
######################
# update E[Y|mp(Y)]
######################
# density ratio before Y
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==outcome])] # M precedes Y
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==outcome])] # L precedes Y
f.M_preY <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede Y = the set M
f.L_preY <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede Y = the set L
# clever coefficient for outcome regression: E(Y|mp(Y))
weight.Y <- if(outcome %in% L){(A==a1)*f.M_preY}else{(A==a0)*1/f.L_preY}
offset.Y <- if(outcome %in% L){mu.Y_a1}else{mu.Y_a0}
if (all(Y %in% c(0,1))){ # binary Y
# one iteration
or_model <- glm(
Y ~ offset(offset.Y)+weight.Y-1, family=binomial(), start=0
)
eps.Y = coef(or_model)
# updated outcome regression
# E[Y|mp(Y)]|_{a1} is updated if Y in L
# E[Y|mp(Y)]|_{a0} is updated if Y in M
if(outcome %in% L){mu.Y_a1 <- plogis(qlogis(offset.Y)+eps.Y*weight.Y)}else{mu.Y_a0 <- plogis(qlogis(offset.Y)+eps.Y*weight.Y)}
} else { # continuous Y
# one iteration
or_model <- lm(
Y ~ offset(offset.Y)+1, weights = weight.Y
)
eps.Y <- coef(or_model)
# updated outcome regression
# E[Y|mp(Y)]|_{a1} is updated if Y in L
# E[Y|mp(Y)]|_{a0} is updated if Y in M
if(outcome %in% L){mu.Y_a1 <- offset.Y+eps.Y}else{mu.Y_a0 <- offset.Y+eps.Y}
}
# update EIF of Y
EIF.Y <- if(outcome %in% L){(A==a1)*f.M_preY*(Y-mu.Y_a1)}else{(A==a0)*1/f.L_preY*(Y-mu.Y_a0)} # if Y in M
######################
# update mu(v,a_v)
######################
for (v in rev(vertices.between.AY)){ ## iterative over the vertices between A and Y to update mu(v,a_v)
#### Update initial estimate of the sequential regression  ###
# vertex that right after Z according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1"))}else{get(paste0("mu.",next.v,"_a0"))}
next.mu.transform <- if(all(Y %in% c(0,1))){qlogis(next.mu)}else{next.mu} # if Y is binary, logit transform the sequential regression first
# Find Markov pillow of v
mpv <- f.markov_pillow(graph, v, treatment) # Markov pillow for outcome
mpv <- replace.vector(mpv, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpv <- data[,mpv] # extract data for Markov pillow for v
if (treatment %in% mpv){ # we only need to consider evaluate regression at specific level of A if treatment is in Markov pillow for v
# set treatment to a0
dat_mpv.a0 <- dat_mpv %>% mutate(!!treatment := a0)
# set treatment to a1
dat_mpv.a1 <- dat_mpv %>% mutate(!!treatment := a1) }
if (crossfit==T){ #### cross fitting + super learner #####
v_fit <- CV.SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), V = K, SL.library = lib.seq, control = list(saveFitLibrary=T),saveAll = T)
v_fit <- .force_weight(v_fit, model="cv", lib=lib.seq) # force weight to be 1 for the one algorithm
#################### prediction: A in mp(v) vs A NOT in mp(v) ######################
if (treatment %in% mpv){
reg_a1 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a1[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
reg_a0 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a0[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
}
} else if (superlearner.seq==T){ #### super learner #####
v_fit <- SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), SL.library = lib.seq)
v_fit <- .force_weight(v_fit, model="sl", lib=lib.seq) # force weight to be 1 for the one algorithm
#################### prediction: A in mp(v) vs A NOT in mp(v) ######################
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)[[1]] %>% as.vector()
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)[[1]] %>% as.vector()
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary)
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
}
}else { #### linear regression #####
v_fit <- lm(next.mu.transform ~ ., data=dat_mpv) # fit linear regression/ logistic regression depending on type of v
#################### prediction: A in mp(v) vs A NOT in mp(v) ######################
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
}
} ## End of update of initial estimates of mu(v,a_v)
##########################################################################################################
## Perform TMLE update for mu(v,a_v)
# offset
if (all(Y %in% c(0,1))){ # L(mu(v)) = mu(next.v)*log(mu(v)(eps.v))+(1-mu(next.v))*log(1-mu(v)(eps.v)), where mu(v)(eps.v)=expit{logit(mu(v))+eps.v*weight}
offset.v <- if(v %in% L){qlogis(get(paste0("mu.",v,"_a1")))}else{qlogis(get(paste0("mu.",v,"_a0")))}
}else{ # L(mu(v)) = weight*{mu(next.v)-mu(v)(eps.v)}^2, where mu(v)(eps.v)=mu(v)+eps.v
offset.v <- if(v %in% L){get(paste0("mu.",v,"_a1"))}else{get(paste0("mu.",v,"_a0"))}}
# weight for regression
# select M and L that precede v
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==v])] # M precedes Z
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==v])] # L precedes Z
weight.v <- if(v %in% L){
# product of the selected variables density ratio
f.M_prev <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede Z
# weight
(A==a1)*f.M_prev }else{
# product of the selected variables density ratio
f.L_prev <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L),drop=F]) # Mi precede Z
# weight
(A==a0)*1/f.L_prev }
# one iteration update
if (all(Y %in% c(0,1))){
v_model <- glm(next.mu ~ offset(offset.v)+weight.v-1, family=binomial(link="logit")) # fit logistic regression if Y is binary, this is like a generalized logistic regression
}else{
v_model <- lm(next.mu ~ offset(offset.v)+1, weights = weight.v) # fit linear regression if Y is continuous
}
# the optimized submodel index
eps.v <- coef(v_model)
# updated outcome regression
# E[Y|mp(Y)]|_{a1} is updated if v in L
# E[Y|mp(Y)]|_{a0} is updated if v in M
if(v %in% L){
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(offset.v+eps.v*weight.v)}else{offset.v+eps.v}) # transform back to probability scale if Y is binary
}else{
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(offset.v+eps.v*weight.v)}else{offset.v+eps.v}) # transform back to probability scale if Y is binary
}
EIF.v <- if(v %in% L){ weight.v*( next.mu - get(paste0("mu.",v,"_a1"))) }else{ weight.v*( next.mu - get(paste0("mu.",v,"_a0")))}
assign(paste0("EIF.",v), EIF.v) # if Y in M
} ## End of update of mu(v,a_v) and EIF.v
# update EIF for A
EIF.A <- ((A==a1) - p.a1.mpA)*get(paste0("mu.",next.A,"_a0"))
# update stoping criteria
EDstar <- mean(EIF.A) + mean(EIF.Y) + mean(rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))))
# update iteration counter
iter <- iter + 1
# record EDstar
EDstar.record <- c(EDstar.record, EDstar)
} ## End of while loop
v_fit$coef
v_fit <- SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), SL.library = lib.seq)
v_fit$coef
sum(v_fit$coef)
ncol(v_fit$coef)
length(lib.seq)
.force_weight <- function(fit, model, lib){
if (length(lib)==1){ # only force weight to be one if there is only one algorithm in lib
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- length(fit$coef) # number of folds
fit$coef <- rep(1, K)
for (i in 1:K){
fit$AllSL[[i]]$coef <- 1
}
}else if (model=="sl"){ # code for force weight when fit is get from SupearLearner
fit$coef <- 1
}
} # only force weight to be one if there is only one algorithm in lib
if (length(lib)>=1){ # if there are more than one learner, and the coefficient for all learners are 0
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- nrow(fit$coef) # number of folds
n.learners <- ncol(fit$coef) # number of learners
for (i in 1:K){
if (sum(fit$AllSL[[i]]$coef)==0){ # if the coefficient for all learners are 0, then force equal weights to all learners
fit$AllSL[[i]]$coef <- rep(1/n.learners,n.learners)
fit$coef[i,] <- rep(1/n.learners,n.learners)
}
}
}else if (model=="sl" & sum(fit$coef)==0){ # code for force weight when fit is get from SupearLearner
n.learners <- length(lib) # number of learners
fit$coef <- rep(1/n.learners,n.learners)
}
} # only force weight to be one if there is only one algorithm in lib
return(fit)
}
v_fit <- .force_weight(v_fit, model="sl", lib=lib.seq) # force weight to be 1 for the one algorithm
head(v_fit)
devtools::build()
