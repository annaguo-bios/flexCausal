dat_mpA <- data[,mpA, drop = F] # extract data for Markov pillow for outcome
if (crossfit==T){ #### cross fitting + super learner #####
# fit model
ps_fit <- CV.SuperLearner(Y=A, X=dat_mpA, family = binomial(), V = K, SL.library = lib.A, control = list(saveFitLibrary=T),saveAll = T)
# make prediction: p(A=1|mp(A))
p.A1.mpA <- ps_fit$SL.predict
} else if (superlearner.A==T){ #### super learner #####
# fit model
ps_fit <- SuperLearner(Y=A, X=dat_mpA, family = binomial(), SL.library = lib.A)
# make prediction: p(A=1|mp(A))
p.A1.mpA <- predict(ps_fit, type = "response")[[1]] %>% as.vector()
} else { # without weak overlapping issue. Run A~X via logistic regression
# fit model
ps_fit <- glm(as.formula(formulaA), data=dat_mpA,  family = binomial())
# make prediction: p(A=1|mp(A))
p.A1.mpA <- predict(ps_fit, type = "response")  # p(A=1|X)
}
}
# apply truncation to propensity score to deal with weak overlap.
# truncated propensity score within the user specified range of [truncate_lower, truncate_upper]: default=[0,1]
p.A1.mpA[p.A1.mpA < truncate_lower] <- truncate_lower
p.A1.mpA[p.A1.mpA > truncate_upper] <- truncate_upper
p.a1.mpA <- a1*p.A1.mpA + (1-a1)*(1-p.A1.mpA) # p(A=a1|mp(A))
p.a0.mpA <-1-p.a1.mpA # p(A=a0|mp(A))
# avoid zero indivision error
p.a0.mpA[p.a0.mpA<zerodiv.avoid] <- zerodiv.avoid
p.a1.mpA[p.a1.mpA<zerodiv.avoid] <- zerodiv.avoid
assign("densratio_A", p.a0.mpA/p.a1.mpA) # density ratio regarding the treatment p(A|mp(A))|_{a_0}/p(A|mp(A))|_{a_1}
ratio.method.L="dnorm" # method for estimating the density ratio associated with M
ratio.method.M="dnorm" # method for estimating the density ratio associated with L
if (ratio.method.L=="densratio"){ ################### METHOD 2A: densratio method  ###################
# Error3: densratio method doesn't support factor variables
if (!all(sapply(replace.vector(unique(c(L.removedA, unlist(lapply(1:length(L.removedA),function(i) f.markov_pillow(graph, L.removedA, treatment))))), multivariate.variables), function(var) is.numeric(data[,var]) | is.integer(data[,var])))){
print("Error in estimating density ratios associated with variables in L: densratio method only support numeric/integer variables, try bayes method instead.")
stop() }
# if M,A,X only consists numeric/integer variables: apply density ratio estimation
for (v in L.removedA){ ## Iterate over each variable in L\A
# used for estimating numerator of the density ratio
dat_v.num.a0 <- data[data[[treatment]] == a0, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)] # select rows where A=a0
dat_v.num.a1 <- data[data[[treatment]] == a1, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)] # select rows where A=a1
# since for v in L, the ratio used is p(L|mp(L))|_{a_1}/p(L|mp(L))|_{a_0}. Therefore, if we calculate the ratio p(L|mp(L))|_{a_0}/p(L|mp(L))|_{a_1} and make it be divided by 1,
# it may result in large values. Therefore, we calculate the ratio p(L|mp(L))|_{a_1}/p(L|mp(L))|_{a_0} and assign it to 1/ratio to make the ratio estimation more stable.
densratio.v.num <- densratio(dat_v.num.a1, dat_v.num.a0) # calculate the ratio of a1/a0
ratio.num <- densratio.v.num$compute_density_ratio(data[, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)])
ratio.num[ratio.num<zerodiv.avoid] <- zerodiv.avoid # control for very small numerator values
## Estimate the denomator via bays rule to avoid zero division error
# used for estimating the denominator of the density ratio
dat_bayes.v <- data[, setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables), treatment), drop=F]
if (crossfit==T){
bayes_fit <- CV.SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), V = K, SL.library = lib.L, control = list(saveFitLibrary=T),saveAll = T)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a1|mp(V))/p(a0|mp(V))
ratio.den <- p.a1.mpv/p.a0.mpv
}else if (superlearner.L==T){
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.L)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a1|mp(V))/p(a0|mp(V))
ratio.den <- p.a1.mpv/p.a0.mpv
} else {
# estimate density ratio using bayes rule
bays_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bays_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a1|mp(V))/p(a0|mp(V))
ratio.den <- p.a1.mpv/p.a0.mpv
}
ratio <- ratio.num/ratio.den # p(L|mp(L))|_{a_1}/p(L|mp(L))|_{a_0}
assign(paste0("densratio_",v), 1/ratio) # but assign ratio a0/a1
}
} else if (ratio.method.L=="dnorm"){ ################### METHOD 2B: dnorm method  ###################
if (!all(sapply(replace.vector(L.removedA, multivariate.variables), function(var) is.numeric(data[,var]) | is.integer(data[,var]) | length(unique(data[,var]))==2))){
print("Error in estimating density ratios associated with variables in L: dnorm method only support continuous or binary variables, try bayes method instead.")
stop() }
# if M,A,X only consists numeric/integer variables: apply density ratio estimation
for (v in L.removedA){ ## Iterate over each variable in L\A
ratio <- calculate_density_ratio_dnorm(a0=a0, v , graph, treatment=treatment, data=data) # p(L|mp(L))|_{a_0}/p(L|mp(L))|_{a_1}
assign(paste0("densratio_",v), ratio)
}
}else if (ratio.method.L=="bayes"){ ################### METHOD 2C: Bayes method ###################
for (v in L.removedA){ ## Iterate over each variable in L\A
#### Prepare data for regression and prediction ####
dat_bayes.v <- data[,setdiff( replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables) ,treatment), drop=F] # contains variable v + Markov pillow of v - treatment
#### Fit nuisance models ####
if (crossfit==T){
bayes_fit <- CV.SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), V = K, SL.library = lib.L, control = list(saveFitLibrary=T),saveAll = T)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
}else if (superlearner.L==T){
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.L)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
} else {
# estimate density ratio using bayes rule
bays_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bays_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
}
} ## Iterate over each variable in L\A
} else {
print("Invalid ratio.method.L input.")
}
# Get all the densratio vectors based on their names
densratio.vectors.L <- mget(c(paste0("densratio_",L.removedA),"densratio_A"))
# Create a data frame using all the vectors
densratio.L <- data.frame(densratio.vectors.L)
M.mpM.includeA <- c()
M.mpM.excludeA <- c()
for (v in M){ ## Iterate over each variable in M
if (treatment %in% f.markov_pillow(graph, v, treatment)){ # vertices whose Markov pillow include A
M.mpM.includeA <- c(M.mpM.includeA, v)
} else { # vertices whose Markov pillow don't include A
M.mpM.excludeA <- c(M.mpM.excludeA, v)
}
} ## Iterate over each variable in M
# assign ratio=1 for vertices in M.mpM.excludeA
for (m in M.mpM.excludeA) { assign(paste0("densratio_", m), 1) }
if (ratio.method.M=="densratio"){ ################### METHOD 2A: densratio method  ###################
# Error: densratio method doesn't support factor variables
if (!all(sapply(replace.vector(unique(c(M.mpM.includeA, unlist(lapply(1:length(M.mpM.includeA), function(i) f.markov_pillow(graph, M.mpM.includeA[i], treatment))))), multivariate.variables), function(var) is.numeric(data[,var]) | is.integer(data[,var])))){
print("Error in estimating density ratios associated with variables in M: densratio method only support numeric/integer variables, try bayes method instead.")
stop() }
# if M and mpi(M) only consists numeric/integer variables: apply density ratio estimation
for (v in M.mpM.includeA){
# used for estimating numerator of the density ratio
dat_v.num.a0 <- data[data[[treatment]] == a0, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)] # select rows where A=a0
dat_v.num.a1 <- data[data[[treatment]] == a1, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)] # select rows where A=a1
densratio.v.num <- densratio(dat_v.num.a0, dat_v.num.a1)
ratio.num <- densratio.v.num$compute_density_ratio(data[, replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables)])
## Estimator the denomator via bayes rule to avoid zero division
# used for estimating the denominator of the density ratio
dat_bayes.v <- data[, setdiff(replace.vector(f.markov_pillow(graph, v, treatment), multivariate.variables), treatment), drop=F]
if (crossfit==T){
bayes_fit <- CV.SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), V = K, SL.library = lib.L, control = list(saveFitLibrary=T),saveAll = T)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
}else if (superlearner.L==T){
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.L)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
} else {
# estimate density ratio using bayes rule
bays_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bays_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(a0|mp(V))/p(a1|mp(V))
ratio.den <- p.a0.mpv/p.a1.mpv
}
ratio <- ratio.num/ratio.den # p(M|mp(M))|_{a_0}/p(M|mp(M))|_{a_1}
assign(paste0("densratio_",v), ratio)
}
} else if (ratio.method.M=="dnorm"){ ################### METHOD 2B: dnorm method  ###################
if (!all(sapply(replace.vector(M.mpM.includeA, multivariate.variables), function(var) is.numeric(data[,var]) | is.integer(data[,var]) | length(unique(data[,var]))==2 ))){
print("Error in estimating density ratios associated with variables in M: dnorm method only support continuous or binary variables, try bayes method instead.")
stop() }
# if M consists of continuous or binary variables: apply density ratio estimation via dnorm
for (v in M.mpM.includeA){ ## Iterate over each variable in L\A
ratio <- calculate_density_ratio_dnorm(a0=a0, v , graph, treatment=treatment, data=data) # p(M|mp(M))|_{a_0}/p(M|mp(M))|_{a_1}
assign(paste0("densratio_",v), ratio)
}
}else if (ratio.method.M=="bayes"){ ################### METHOD 2C: Bayes method ###################
for (v in M.mpM.includeA){ ## Iterate over each variable in M
#### Prepare data for regression and prediction ####
dat_bayes.v <- data[,setdiff(replace.vector(c(v, f.markov_pillow(graph, v, treatment)), multivariate.variables) ,treatment), drop=F] # contains variable v + Markov pillow of v - treatment
#### Fit nuisance models ####
if (crossfit==T){
# fit p(A|mp(v)\A,v)
bayes_fit <- CV.SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), V = K, SL.library = lib.M, control = list(saveFitLibrary=T),saveAll = T)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- bayes_fit$SL.predict
#p(A=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(A=a1|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
save(list=c("bayes_fit","p.a0.mpv","p.a1.mpv"), file=paste0("bayes_fit_cf_",v,".RData"))
}else if (superlearner.M==T){
# fit p(A|mp(v)\A,v)
bayes_fit <- SuperLearner(Y=A, X=dat_bayes.v, family = binomial(), SL.library = lib.L)
# p(A=1|mp(v)\A,v)
p.A1.mpv <- predict(bayes_fit, type = "response")[[1]] %>% as.vector()  # p(A=1|X)
#p(A=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.A1.mpv+(1-a0)*(1-p.A1.mpv)
#p(A=a1|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
} else {
# estimate density ratio using bayes rule
bays_fit <- glm(A ~ ., data=dat_bayes.v, family = binomial())
# p(A=1|mp(v)\A,v)
p.a1.mpv <- predict(bays_fit, type = "response")
#p(v=a0|mp(v)\A,v)
p.a0.mpv <- a0*p.a1.mpv+(1-a0)*(1-p.a1.mpv)
#p(v=a0|mp(v)\A,v)
p.a1.mpv <- 1-p.a0.mpv
p.a1.mpv[p.a1.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
p.a0.mpv[p.a0.mpv<zerodiv.avoid] <- zerodiv.avoid # added to avoid INF
# save the density ratio: p(v|mp(V))|_{a0}/p(v|mp(V))|_{a1}
assign(paste0("densratio_",v), {p.a0.mpv/p.a1.mpv}/densratio_A)
# save the ratio of p(A|mp(v)\A,v)|_{a0}/p(A|mp(v)\A,v)|_{a1}
# such that we can come back to update the density ratio of v once we update the densratioA
assign(paste0("bayes.densratio_",v), {p.a0.mpv/p.a1.mpv})
}
} ## Iterate over each variable in M
} else {
print("Invalid ratio.method.M input.")
stop()
}
# Get the densratio vectors based on their names
densratio.vectors.M <- mget(paste0("densratio_",M))
# Create a data frame using all the vectors
densratio.M <- data.frame(densratio.vectors.M)
# the sequential regression will be perform for v that locates between A and Y accroding to topological order tau
vertices.between.AY <- tau[{which(tau==treatment)+1}:{which(tau==outcome)-1}]
# sequential regression
for (v in rev(vertices.between.AY)){ ## iterate through vertices between A and Y according to topological order tau
# perform sequential regressions for v with larger order first: rev()
### Prepare Markov pillow and dataset for regression and prediction ####
# Find Markov pillow of v
mpv <- f.markov_pillow(graph, v, treatment) # Markov pillow for outcome
mpv <- replace.vector(mpv, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpv <- data[,mpv] # extract data for Markov pillow for v
if (treatment %in% mpv){ # we only need to consider evaluate regression at specific level of A if treatment is in Markov pillow for v
# set treatment to a0
dat_mpv.a0 <- dat_mpv %>% mutate(!!treatment := a0)
# set treatment to a1
dat_mpv.a1 <- dat_mpv %>% mutate(!!treatment := a1) }
# vertex that right after Z according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1")) }else{ get(paste0("mu.",next.v,"_a0")) } # mu is evaluated at a0 for next.v in M and at a1 for next.v in L
next.mu.transform <- if(all(Y %in% c(0,1))){qlogis(next.mu)}else{next.mu} # if Y is binary, logit transform the sequential regression first
### End of preparation ####
if (crossfit==T){ #### cross fitting + super learner #####
v_fit <- CV.SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), V = K, SL.library = lib.seq, control = list(saveFitLibrary=T), saveAll = T)
######## prediction: A in mp(v) vs A NOT in mp(v) ########
if (treatment %in% mpv){
reg_a1 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a1[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
reg_a0 <- unlist(lapply(1:K, function(x) predict(v_fit$AllSL[[x]], newdata=dat_mpv.a0[v_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) v_fit$folds[[x]])))]
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(v_fit$SL.predict)}else{v_fit$SL.predict}) # transform back to probability scale if Y is binary
}
#########################################################
} else if (superlearner.seq==T){ #### super learner #####
v_fit <- SuperLearner(Y=next.mu.transform, X=dat_mpv, family = gaussian(), SL.library = lib.seq)
######## prediction: A in mp(v) vs A NOT in mp(v) ########
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)[[1]] %>% as.vector()
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)[[1]] %>% as.vector()
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary)
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit)[[1]] %>% as.vector())}else{predict(v_fit)[[1]] %>% as.vector()}) # transform back to probability scale if Y is binary
}
#########################################################
}else { #### linear regression #####
v_fit <- lm(next.mu.transform ~ ., data=dat_mpv) # fit linear regression/ logistic regression depending on type of v
######## prediction: A in mp(v) vs A NOT in mp(v)########
if (treatment %in% mpv){
reg_a1 <- predict(v_fit, newdata=dat_mpv.a1)
reg_a0 <- predict(v_fit, newdata=dat_mpv.a0)
# assign prediction as mu.v_a1 and mu.v_a0
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(reg_a1)}else{reg_a1}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(reg_a0)}else{reg_a0}) # transform back to probability scale if Y is binary
}else{
# assign prediction as mu.v_a1 and mu.v_a0, where mu.v_a1 = mu.v_a0 = mu.v
assign(paste0("mu.",v,"_a1"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
assign(paste0("mu.",v,"_a0"), if(all(Y %in% c(0,1))){plogis(predict(v_fit))}else{predict(v_fit)}) # transform back to probability scale if Y is binary
}
##########################################################
}
}  ## End of iteration over all vertics between A and Y
# density ratio before Y
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==outcome])] # M precedes Y
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==outcome])] # L precedes Y
f.M_preY <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede Y = the set M
f.L_preY <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede Y = the set L
EIF.Y <- if(outcome %in% L){
(A==a1)*f.M_preY*(Y-mu.Y_a1)}else{ # if Y in L
(A==a0)*1/f.L_preY*(Y-mu.Y_a0)} # if Y in M
rev(vertices.between.AY)
v="L"
# select M and L that precede v
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==v])] # M precedes Z
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==v])] # L precedes Z
# vertex that right after v according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
next.v
# mu(next.v, a_{next.v})
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1"))}else{get(paste0("mu.",next.v,"_a0"))}
selected.L
summary(ps_fit)
# product of the selected variables density ratio
f.L_prev <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede v
head(f.L_prev)
for (v in rev(vertices.between.AY)){ ## iterate over all vertices between A and Y
# select M and L that precede v
selected.M <- M[sapply(M, function(m) tau.df$order[tau.df$tau==m] < tau.df$order[tau.df$tau==v])] # M precedes Z
selected.L <- L[sapply(L, function(l) tau.df$order[tau.df$tau==l] < tau.df$order[tau.df$tau==v])] # L precedes Z
# vertex that right after v according to tau
next.v <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==v]+1}]
# mu(next.v, a_{next.v})
next.mu <- if(next.v %in% L){ get(paste0("mu.",next.v,"_a1"))}else{get(paste0("mu.",next.v,"_a0"))}
EIF.v <- if(v %in% L){ # v in L
# product of the selected variables density ratio
f.M_prev <- Reduce(`*`, densratio.M[,paste0("densratio_",selected.M), drop=F]) # Mi precede v
# EIF for v|mp(v)
(A==a1)*f.M_prev*( next.mu - get(paste0("mu.",v,"_a1")) )
}else{ # v in M
# product of the selected variables density ratio
f.L_prev <- Reduce(`*`, densratio.L[,paste0("densratio_",selected.L), drop=F]) # Li precede v
# EIF for v|mp(v)
(A==a0)*1/f.L_prev*( next.mu - get(paste0("mu.",v,"_a0")) )
}
assign(paste0("EIF.",v), EIF.v)
} ## End of iteration over all vertices between A and Y
# vertex that right after A according to tau
next.A <- tau.df$tau[tau.df$order=={tau.df$order[tau.df$tau==treatment]+1}]
EIF.A <- {(A==a1) - p.a1.mpA}*get(paste0("mu.",next.A,"_a0"))
# estimated psi
estimated_psi = mean( EIF.Y + rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))) +  EIF.A + p.a1.mpA*get(paste0("mu.",next.A,"_a0")) + (A==a0)*Y )
# EIF
EIF <- EIF.Y + # EIF of Y|mp(Y)
rowSums(as.data.frame(mget(paste0("EIF.",vertices.between.AY)))) + # EIF of v|mp(v) for v between A and Y
EIF.A + # EIF of A|mp(A)
p.a1.mpA*get(paste0("mu.",next.A,"_a0")) + # EIF of mp(A)
mean((A==a0)*Y) -
estimated_psi
mean(EIF^2)
mean(EIF.Y)
truth.M <- as.matrix(data.frame("M"=p.M.a0X/p.M.a1X, "L"=p.L.Ma0X/p.L.Ma1X))
parM = matrix(c(1, 1, 1, 0,-1,-0.5,2,0), nrow = 2,byrow = T)
parL= c(1,1,1, 1, 1)
sd.M= matrix(c(2, 1, 1, 3), nrow = 2)
sd.L = 1
density.m.ax <- function(parM, m, a, x, sd.M) {
dmvnorm(m, mean = c(parM[1, 1] + parM[1, 2] * a + parM[1, 3] * x + parM[1, 4] * a * x,
parM[2, 1] + parM[2, 2] * a + parM[2, 3] * x + parM[2, 4] * a * x), sigma = sd.M)
}
p.M.a1X <- sapply(1:n, function(i) {density.m.ax(parM, c(M.1[i], M.2[i]), a1, X[i], sd.M)}) # p(M|a,X)
parM = matrix(c(1, 1, 1, 0,-1,-0.5,2,0), nrow = 2,byrow = T)
parL= c(1,1,1, 1, 1)
sd.M= matrix(c(2, 1, 1, 3), nrow = 2)
sd.L = 1
density.m.ax <- function(parM, m, a, x, sd.M) {
dmvnorm(m, mean = c(parM[1, 1] + parM[1, 2] * a + parM[1, 3] * x + parM[1, 4] * a * x,
parM[2, 1] + parM[2, 2] * a + parM[2, 3] * x + parM[2, 4] * a * x), sigma = sd.M)
}
p.M.a1X <- sapply(1:n, function(i) {density.m.ax(parM, c(M1[i], M2[i]), a1, X[i], sd.M)}) # p(M|a,X)
attach(data_fig_4a)
head(data)
parM = matrix(c(1, 1, 1, 0,-1,-0.5,2,0), nrow = 2,byrow = T)
parL= c(1,1,1, 1, 1)
sd.M= matrix(c(2, 1, 1, 3), nrow = 2)
sd.L = 1
density.m.ax <- function(parM, m, a, x, sd.M) {
dmvnorm(m, mean = c(parM[1, 1] + parM[1, 2] * a + parM[1, 3] * x + parM[1, 4] * a * x,
parM[2, 1] + parM[2, 2] * a + parM[2, 3] * x + parM[2, 4] * a * x), sigma = sd.M)
}
p.M.a1X <- sapply(1:n, function(i) {density.m.ax(parM, c(data$M1[i], data$M2[i]), a1, data$X[i], sd.M)}) # p(M|a,X)
p.M.a0X <- sapply(1:n, function(i) {density.m.ax(parM, c(data$M1[i], dataM2[i]), a0, data$X[i], sd.M)}) # p(M|A,X)
# true p(L|M,A,X)
density.l.max <- function(parL, l, m, a, x, sd.L) {
dnorm(l, mean = parL[1] + parL[2] * a + parL[3] * m[1] + parL[4] * m[2] + parL[5] * x, sd.L)
}
p.M.a0X <- sapply(1:n, function(i) {density.m.ax(parM, c(data$M1[i], data$M2[i]), a0, data$X[i], sd.M)}) # p(M|A,X)
# true p(L|M,A,X)
density.l.max <- function(parL, l, m, a, x, sd.L) {
dnorm(l, mean = parL[1] + parL[2] * a + parL[3] * m[1] + parL[4] * m[2] + parL[5] * x, sd.L)
}
# L|M,A,X
p.L.Ma1X <- sapply(1:n, function(i) {density.l.max(parL, data$L[i], c(data$M1[i], data$M2[i]), a1, data$X[i], sd.L)}) # p(L|a,X)
p.L.Ma0X <- sapply(1:n, function(i) {density.l.max(parL, data$L[i], c(data$M1[i], data$M2[i]), a0, data$X[i], sd.L)}) # p(L|A,X)
truth.M <- as.matrix(data.frame("M"=p.M.a0X/p.M.a1X, "L"=p.L.Ma0X/p.L.Ma1X))
oldEIF <- EIF
EIF.Y <- (A==a1)*truth.M*(Y-mu.Y_a1)
mean(oldEIF^2)
mean(EIF^2)
head(dat_mpA)
formulaA
# fit model
ps_fit <- glm(as.formula(formulaA), data=dat_mpA,  family = binomial())
summary(ps_fit)
generate_data <- function(n,parA = c(1,1), parU=c(1,1,1,0), parM = matrix(c(1, 1, 1, 0,-1,-0.5,2,0), nrow = 2,byrow = T), parL= c(1,1,1, 1, 1) ,parY = c(1, 1, 1, 1, 1, 1), sd.U=1, sd.M= matrix(c(2, 1, 1, 3), nrow = 2), sd.L=1, sd.Y=1){
X <- runif(n, 0, 1) # p(X)
A <- rbinom(n, 1, plogis(parA[1] + parA[2]*X)) # p(A|X)
U <- parU[1] + parU[2]*A + parU[3]*X + parU[4]*A*X + rnorm(n,0,sd.U) # p(U|A,X)
M <- cbind(parM[1,1] + parM[1,2]*A + parM[1,3]*X + parM[1,4]*A*X,
parM[2,1] + parM[2,2]*A + parM[2,3]*X + parM[2,4]*A*X)+ mvrnorm(n , mu =c(0,0) , Sigma = sd.M) # p(M|A,X)
L <- parL[1] + parL[2]*A + parL[3]*M[,1] + parL[4]*M[,2] + parL[5]*X + rnorm(n,0,sd.L) # p(L|A,X)
Y <- parY[1] + parY[2]*L  + parY[3]*M[,1] + parY[4]*M[,2] + parY[5]*X + parY[6]*U  + rnorm(n, 0, sd.Y) # p(Y|U,M,X)
data <- data.frame(X=X, U=U, A=A, M1=M[,1], M2=M[,2], L=L, Y=Y)
# propensity score
ps <- A*(parA[1] + parA[2]*X)+(1-A)*(1-(parA[1] + parA[2]*X))
return(list(data = data,
parA=parA,
parU=parU,
parM=parM,
parL=parL,
parY=parY,
sd.U=sd.U,
sd.M=sd.M,
sd.L=sd.L,
sd.Y=sd.Y,
ps=ps))
}
data <- generate_data(2000)$data
library(MASS)
data <- generate_data(2000)$data
A <- data[,"A"]
# Find Markov pillow of treatment
mpA <- f.markov_pillow(graph, node=treatment, treatment=treatment) # Markov pillow for treatment
mpA <- replace.vector(mpA, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpA <- data[,mpA, drop = F] # extract data for Markov pillow for outcome
# fit model
ps_fit <- glm(as.formula(formulaA), data=dat_mpA,  family = binomial())
summary(ps_fit)
