mp <- f.markov_pillow(graph, M, treatment=treatment) # Markov pillow for M
M="M"
mp <- f.markov_pillow(graph, M, treatment=treatment) # Markov pillow for M
mp <- replace.vector(mp, multivariate.variables) # replace multivariate vertices with their elements
# prediction data
data.a0 <- data[, mp] %>% mutate(!!treatment := a0)
library(dplyr)
# prediction data
data.a0 <- data[, mp] %>% mutate(!!treatment := a0)
data.a1 <- data[, mp] %>% mutate(!!treatment := (1-a0))
# for which variables, user specified formula for regression
if (!is.null(formula)){
formula.variables <- names(formula)
})
# for which variables, user specified formula for regression
if (!is.null(formula)){
formula.variables <- names(formula)
}
formula.variables
M %in% names(multivariate.variables)
num_columns <- length(multivariate.variables[[M]]) # number of variables under vertex M
variables <- multivariate.variables[[M]]
variables
for (var in variables){
if (all(data[, var] %in% c(0,1))) {
stop("This function doesn't support multivariate variables with binary elements.")
}
} # end of for loop over variables
# Initialize an empty matrix to store the fitted coefficients
fit.parM <- list() # store the coefficients of the model for each variable in M
predict.M.a0 <- list() # store the prediction results for each variable in M under A=a0
predict.M.a1 <- list() # store the prediction results for each variable in M under A=a1
# Initialize vectors to store errors
model_errors <- matrix(NA, nrow = num_rows, ncol = num_columns)
for (i in 1:num_columns){ # loop over each variable in M
if (variables[i] %in% formula.variables){ # if the user specified formula for this variable
model <- lm(formula[[variables[i]]], data=data[, c(variables[i],mp)])
}else{ # if the user didn't specify formula for this variable
model <- lm(data[, variables[i]] ~ . , data=data[, mp])
}
fit.parM[[variables[i]]] <- coef(model) # store the coefficients of the model
model_errors[, i] <- data[, variables[i]] - predict(model) # errors of the model
predict.M.a0[[variables[i]]] <- predict(model, newdata=data.a0) # store the prediction result for A=a0
predict.M.a1[[variables[i]]] <- predict(model, newdata=data.a1) # store the prediction result for A=a1
} # end of for loop over variables
i=1
variables[i]
variables[i] %in% formula.variables
formula[[variables[i]]]
mp
formula
formula[[1]] <- "M.1 ~ A + A*X"
formula
model <- lm(as.formula(formula[[variables[i]]]), data=data[, c(variables[i],mp)])
if (variables[i] %in% formula.variables){ # if the user specified formula for this variable
model <- lm(as.formula(formula[[variables[i]]]), data=data[, c(variables[i],mp)])
}else{ # if the user didn't specify formula for this variable
model <- lm(data[, variables[i]] ~ . , data=data[, mp])
}
fit.parM[[variables[i]]] <- coef(model) # store the coefficients of the model
coef(model)
model_errors[, i] <- data[, variables[i]] - predict(model) # errors of the model
predict.M.a0[[variables[i]]] <- predict(model, newdata=data.a0) # store the prediction result for A=a0
predict.M.a1[[variables[i]]] <- predict(model, newdata=data.a1) # store the prediction result for A=a1
# compute the variance-covariance matrix of the errors
varcov <- cov(data.frame(model_errors))
head(predict.M.a0[["M/1"]])
head(predict.M.a0[["M.1"]])
head(predict.M.a1[["M.1"]])
# Define a function for the ratio calculation
# Define a function for the ratio calculation
f.m.ratio.a <- function(j) {
mean.a0 <- sapply(predict.M.a0, `[`, j) # mean of the normal distribution for A=a0
mean.a1 <- sapply(predict.M.a1, `[`, j) # mean of the normal distribution for A=a1
dmvnorm(
x = data[j, variables],
mean = mean.a0, # coeff*mp
sigma = varcov
) / dmvnorm(
x = data[j, variables],
mean = mean.a1, # coeff*mp
sigma = varcov
)
}
# Apply the function to each row of M
ratio <- sapply(1:num_rows, f.m.ratio.a)
j=1
mean.a0 <- sapply(predict.M.a0, `[`, j) # mean of the normal distribution for A=a0
mean.a0
mean.a1 <- sapply(predict.M.a1, `[`, j) # mean of the normal distribution for A=a1
mean.a1
names(predict.M.a0)
# Initialize an empty matrix to store the fitted coefficients
#fit.parM <- list() # store the coefficients of the model for each variable in M
predict.M.a0 <- list() # store the prediction results for each variable in M under A=a0
predict.M.a1 <- list() # store the prediction results for each variable in M under A=a1
# Initialize vectors to store errors
model_errors <- matrix(NA, nrow = num_rows, ncol = num_columns)
for (i in 1:num_columns){ # loop over each variable in M
if (variables[i] %in% formula.variables){ # if the user specified formula for this variable
model <- lm(as.formula(formula[[variables[i]]]), data=data[, c(variables[i],mp)])
}else{ # if the user didn't specify formula for this variable
model <- lm(data[, variables[i]] ~ . , data=data[, mp])
}
fit.parM[[variables[i]]] <- coef(model) # store the coefficients of the model
model_errors[, i] <- data[, variables[i]] - predict(model) # errors of the model
predict.M.a0[[variables[i]]] <- predict(model, newdata=data.a0) # store the prediction result for A=a0
predict.M.a1[[variables[i]]] <- predict(model, newdata=data.a1) # store the prediction result for A=a1
} # end of for loop over variables
# compute the variance-covariance matrix of the errors
varcov <- cov(data.frame(model_errors))
names(predict.M.a0)
mean.a0 <- sapply(predict.M.a0, `[`, j) # mean of the normal distribution for A=a0
mean.a0
# Define a function for the ratio calculation
# Define a function for the ratio calculation
f.m.ratio.a <- function(j) {
mean.a0 <- sapply(predict.M.a0, `[`, j) # mean of the normal distribution for A=a0
mean.a1 <- sapply(predict.M.a1, `[`, j) # mean of the normal distribution for A=a1
dmvnorm(
x = data[j, variables],
mean = mean.a0, # coeff*mp
sigma = varcov
) / dmvnorm(
x = data[j, variables],
mean = mean.a1, # coeff*mp
sigma = varcov
)
}
# Apply the function to each row of M
ratio <- sapply(1:num_rows, f.m.ratio.a)
head(ratio)
M <- "M.1"
M %in% formula.variables
model <- lm(as.formula(formula[[M]]), data=data[, c(mp,M)])
predict.M.a0 <- predict(model, newdata=data.a0) # store the prediction result for A=a0
predict.M.a1 <- predict(model, newdata=data.a1) # store the prediction result for A=a1
# Store model errors
model_errors <- data[, M] - predict(model)
ratio <- dnorm(as.vector(data[,M]), mean = predict.M.a0, sd = sd(model_errors))/
dnorm(as.vector(data[,M]), mean = predict.M.a1, sd = sd(model_errors))
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::build()
1 < NULL
tmp <- 1:4
tmp[tmp<NULL] <- 10
tmp
eval(parse('list(L = 'L ~ M.1 + M.2 + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2) + A + I(A*X.1)' )'))
eval(parse("list(L = 'L ~ M.1 + M.2 + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2) + A + I(A*X.1)' )"))
library(MASS)
library(mvtnorm)
generate_data <- function(n,parA = c(0.48, 0.07, 1.00, -1.00, -0.34, -0.12, 0.30, -0.35, 1.00, -0.10, 0.46, # linear terms
0.33, 0.00, 0.45, 0.1, -0.32, -0.08, -0.2, 0.50, 0.50, -0.03)*0.1,
parU1=c(1,1,1,0),parU2=c(1,1,1,1,1),
parM = matrix(c(3.0, 1.5, -1.5, -1.5, -1.0, -2.0, -3.0, -3.0, -1.5, 2.0, 1.5, 3.0, # linear terms
1.5, 2.0, 0.5, 0.5, 3.0, # A*X interactions
-0.2, -0.33, 0.5, 0.3, -0.5,# X^2 high order terms
1.5, -1.5, -3.0, 2.0, -2.0, 3.0, -3.0, 1.5, -1.5, -1.5, 1.5, -1.0, # linear terms
-1.5, 0.3, 3.0, -0.33, 0.5, # A*X interactions
0.5, 0.5, -0.2, 0.1, 0.2)*0.025, nrow=2, byrow = T), # X^2 high order terms
parL= c(-3.0, -2.0, -1.5, 1.5, -1.5, -1.0, 0.5, -1.0, 0.3, 3.0, 0.5, 1.5, 0.5, -1.5, # linear terms
-3.0, -0.5, 0.5, 3.0, 1.5)*0.025 , # X^2 high order terms
parY = c(1.0, -2.0, -3.0, -1.5, 1.0, 0.5, -2.0, 1.5, -2.0, -3.0, -3.0, -1.5, -1.0, 0.5, 3.0, # linear terms
1.0, 1.5, -2.0, 3.0),
sd.U1=1, sd.U2=1, sd.M= matrix(c(2, 1, 1, 3), nrow = 2), sd.L=1, sd.Y=1){
# generate X from uniform distr
X <- replicate(10, runif(n,0,1))
A <- rbinom(n, 1, plogis(parA[1] + rowSums(sweep(X, 2, parA[2:11], "*")) +  rowSums(sweep(X^2,2,parA[12:21],"*")) )) # p(A|X)
U1 <- parU1[1] + parU1[2]*A + parU1[3]*X[,1] + parU1[4]*A*X[,1] + rnorm(n,0,sd.U1) # p(U1|A,X)
M <- cbind(parM[1,1] + parM[1,2]*A + rowSums(sweep(X, 2, parM[1,3:12], "*")) + A*rowSums(sweep(X[,1:5],2,parM[1,13:17],"*")) + rowSums(sweep(X[,6:10]^2,2,parM[1,18:22],"*")),
parM[2,1] + parM[2,2]*A + rowSums(sweep(X, 2, parM[2,3:12], "*")) + A*rowSums(sweep(X[,1:5],2,parM[2,13:17],"*")) + rowSums(sweep(X[,6:10]^2,2,parM[2,18:22],"*")))+ mvrnorm(n , mu =c(0,0) , Sigma = sd.M) # p(M|A,X)
U2 <- parU2[1] + parU2[2]*M[,1] + parU2[3]*M[,2] + parU2[4]*A + parU2[5]*X[,1] + rnorm(n,0,sd.U2) # p(U2|A,X,M)
L <- parL[1] + parL[2]*M[,1] + parL[3]*M[,2] + rowSums(sweep(X, 2, parL[4:13], "*"))  + rowSums(sweep(X[,6:10]^2,2,parL[14:18],"*")) + parL[19]*U1 + rnorm(n,0,sd.L) # p(L|M,X,U1)
Y <- parY[1] + parY[2]*L  + parY[3]*A + rowSums(sweep(X, 2, parY[4:13], "*"))  + rowSums(sweep(X[,6:10]^2,2,parY[14:18],"*")) + parY[19]*U2  + rnorm(n, 0, sd.Y) # p(Y|L,A,X,U2)
data <- data.frame(X=X, U1=U1, U2=U2, A=A, M=M, L=L, Y=Y)
return(list(data = data,
parA=parA,
parU1=parU1,
parU2=parU2,
parM=parM,
parL=parL,
parY=parY,
sd.U1=sd.U1,
sd.U2=sd.U2,
sd.M=sd.M,
sd.L=sd.L,
sd.Y=sd.Y))
}
data <- generate_data(500)$data
library(ADMGtmle)
library(ggplot2)
library(ggpubr)
library(latex2exp)
library(reshape2)
library(stats)
library(haldensify)
library(np)
library(xtable)
library(SuperLearner)
library(densratio)
library(MASS)
library(mvtnorm)
library(ADMGtmle)
library(utils)
graph=NULL
a =  1
vertices = c("A", "M", "L", "Y", "X") # vertices
di_edges =  list(c("X", "A"), c("X", "M"), c("X", "L"), c("X", "Y"), c("A", "M"), c("M", "L"), c("L", "Y"), c("A", "Y"))
# directed edges
bi_edges = list(c("A", "L"), c("M", "Y")) # bidirected edges
treatment = "A" # name of the treatment variable
outcome = "Y" # name of the outcome variable
multivariate.variables = list(M = c("M.1", "M.2"), X=c("X.1","X.2","X.3","X.4","X.5","X.6","X.7","X.8","X.9","X.10"))  # name of the multivariate variables
ratio.method.L="dnorm" # method for estimating the density ratio associated with M
ratio.method.M="dnorm" # method for estimating the density ratio associated with L
lib.seq = c('SL.ranger') # superlearner lib for sequential regression
lib.L = c('SL.ranger') # superlearner lib for L via bayes
lib.M = c('SL.ranger') # superlearner lib for M via bayes
lib.Y = c('SL.ranger') # superlearner lib for Y
lib.A = c('SL.ranger') # superlearner lib for A
superlearner.seq = T # whether to use superlearner for sequential regression
superlearner.L = T # whether to use superlearner for L
superlearner.M = T # whether to use superlearner for M
superlearner.Y = T # whether to use superlearner for Y
superlearner.A = T # whether to use superlearner for A
formulaY="Y ~ ."
formulaA="A ~ ." # regression formula for outcome regression and propensity score if superlearner is not used
linkY_binary="logit"
linkA="logit" # link function for outcome regression and propensity score if superlearner is not used
n.iter=500
cvg.criteria=0.01
truncate_lower=0
truncate_upper=1
zerodiv.avoid=0
K=2
crossfit=F
dnorm.formua.L <- list(L = 'L ~ M.1 + M.2 + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2) + A + I(A*X.1)' )
dnorm.formua.M <- list(M.1 = 'M.1 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)', M.2 = 'M.2 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)')
n <- nrow(data)
a0 <- a
a1 <- 1-a
# extract graph components
if (!is.null(graph)){
vertices <- graph$vertices
di_edges <- graph$di_edges
bi_edges <- graph$bi_edges
multivariate.variables <- graph$multivariate.variables
}else{
# make a graph object
graph <- make.graph(vertices=vertices, bi_edges=bi_edges, di_edges=di_edges, multivariate.variables=multivariate.variables)
}
# return topological ordering
tau <- f.top_order(graph, treatment)
tau.df <- data.frame(tau=tau, order = 1:length(tau))
# Get set C, M, L
setCML <- CML(graph, treatment) # get set C, M, L
C <- setCML$C # everything comes before the treatment following topological order tau
L <- setCML$L # variables within the district of treatment and comes after the treatment (including the treatment itself) following topological order tau
M <- setCML$M # everything else
C <- rerank(C, tau) # re-order vertices in C according to their topological order in tau
L <- rerank(L, tau) # re-order vertices in L according to their topological order in tau
L.removedA <- L[L!=treatment] # remove treatment from L
M <- rerank(M, tau) # re-order vertices in M according to their topological order in tau
# Variables
A <- data[,treatment] # treatment
Y <- data[,outcome] # outcome
are_same <- function(vec1, vec2) {
identical(sort(vec1), sort(vec2))
}
crossfit=T
# Find Markov pillow of outcome
mpY <- f.markov_pillow(graph, node=outcome, treatment=treatment) # Markov pillow for outcome
mpY <- replace.vector(mpY, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpY <- data[,mpY] # extract data for Markov pillow for outcome
dat_mpY.a0 <- dat_mpY %>% mutate(!!treatment := a0) # set treatment to a0
dat_mpY.a1 <- dat_mpY %>% mutate(!!treatment := a1) # set treatment to a1
if (crossfit==T){ #### cross fitting + super learner #####
fit.family <- if(all(Y %in% c(0,1))){binomial(linkY_binary)}else{gaussian()} # family for super learner depending on whether Y is binary or continuous
or_fit <- CV.SuperLearner(Y=Y, X=dat_mpY, family = fit.family, V = K, SL.library = lib.Y, control = list(saveFitLibrary=T),saveAll = T)
or_fit <- .force_weight(or_fit, model="cv", lib=lib.Y) # force weight to be 1 for the one algorithm
mu.Y_a1 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=dat_mpY.a1[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
mu.Y_a0 <- unlist(lapply(1:K, function(x) predict(or_fit$AllSL[[x]], newdata=dat_mpY.a0[or_fit$folds[[x]],])[[1]] %>% as.vector()))[order(unlist(lapply(1:K, function(x) or_fit$folds[[x]])))]
} else if (superlearner.Y==T){ #### super learner #####
fit.family <- if(all(Y %in% c(0,1))){binomial(linkY_binary)}else{gaussian()} # family for super learner depending on whether Y is binary or continuous
or_fit <- SuperLearner(Y=Y, X=dat_mpY, family = fit.family, SL.library = lib.Y)
or_fit <- .force_weight(or_fit, model="sl", lib=lib.Y) # force weight to be 1 for the one algorithm
mu.Y_a1 <- predict(or_fit, newdata=dat_mpY.a1)[[1]] %>% as.vector()
mu.Y_a0 <- predict(or_fit, newdata=dat_mpY.a0)[[1]] %>% as.vector()
} else { #### simple linear regression with user input regression formula: default="Y ~ ." ####
fit.family <- if(all(Y %in% c(0,1))){binomial(linkY_binary)}else{gaussian()} # family for super learner depending on whether Y is binary or continuous
or_fit <- glm(as.formula(formulaY), data=dat_mpY, family = fit.family)
mu.Y_a1 <- predict(or_fit, newdata=dat_mpY.a1)
mu.Y_a0 <- predict(or_fit, newdata=dat_mpY.a0)
}
# Find Markov pillow of outcome
mpY <- f.markov_pillow(graph, node=outcome, treatment=treatment) # Markov pillow for outcome
mpY <- replace.vector(mpY, multivariate.variables) # replace vertices with it's components if vertices are multivariate
# prepare dataset for regression and prediction
dat_mpY <- data[,mpY] # extract data for Markov pillow for outcome
dat_mpY.a0 <- dat_mpY %>% mutate(!!treatment := a0) # set treatment to a0
dat_mpY.a1 <- dat_mpY %>% mutate(!!treatment := a1) # set treatment to a1
fit.family <- if(all(Y %in% c(0,1))){binomial(linkY_binary)}else{gaussian()} # family for super learner depending on whether Y is binary or continuous
or_fit <- CV.SuperLearner(Y=Y, X=dat_mpY, family = fit.family, V = K, SL.library = lib.Y, control = list(saveFitLibrary=T),saveAll = T)
or_fit <- .force_weight(or_fit, model="cv", lib=lib.Y) # force weight to be 1 for the one algorithm
fit <- or_fit
model="cv"
lib=lb.Y
lib=lib.Y
length(lib)==1
model=="cv"
K <- length(fit$coef) # number of folds
K
fit$coef <- rep(1, K)
for (i in 1:K){
fit$AllSL[[i]]$coef <- 1
}
.force_weight <- function(fit, model, lib){
if (length(lib)==1){ # only force weight to be one if there is only one algorithm in lib
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- length(fit$coef) # number of folds
fit$coef <- rep(1, K)
for (i in 1:K){
fit$AllSL[[i]]$coef <- 1
}
}else if (model=="sl"){ # code for force weight when fit is get from SupearLearner
fit$coef <- 1
}
} # only force weight to be one if there is only one algorithm in lib
if (length(lib)>=1){ # if there are more than one learner, and the coefficient for all learners are 0
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- nrow(fit$coef) # number of folds
n.learners <- ncol(fit$coef) # number of learners
for (i in 1:K){
if (sum(fit$AllSL[[i]]$coef)==0){ # if the coefficient for all learners are 0, then force equal weights to all learners
fit$AllSL[[i]]$coef <- rep(1/n.learners,n.learners)
fit$coef[i,] <- rep(1/n.learners,n.learners)
}
}
}else if (model=="sl" & sum(fit$coef)==0){ # code for force weight when fit is get from SupearLearner
n.learners <- length(lib) # number of learners
fit$coef <- rep(1/n.learners,n.learners)
}
} # only force weight to be one if there is only one algorithm in lib
return(fit)
}
or_fit <- .force_weight(or_fit, model="cv", lib=lib.Y) # force weight to be 1 for the one algorithm
if (length(lib)==1){ # only force weight to be one if there is only one algorithm in lib
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- length(fit$coef) # number of folds
fit$coef <- rep(1, K)
for (i in 1:K){
fit$AllSL[[i]]$coef <- 1
}
}else if (model=="sl"){ # code for force weight when fit is get from SupearLearner
fit$coef <- 1
}
} # only force weight to be one if there is only one algorithm in lib
if (length(lib)>=1){ # if there are more than one learner, and the coefficient for all learners are 0
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- nrow(fit$coef) # number of folds
n.learners <- ncol(fit$coef) # number of learners
for (i in 1:K){
if (sum(fit$AllSL[[i]]$coef)==0){ # if the coefficient for all learners are 0, then force equal weights to all learners
fit$AllSL[[i]]$coef <- rep(1/n.learners,n.learners)
fit$coef[i,] <- rep(1/n.learners,n.learners)
}
}
}else if (model=="sl" & sum(fit$coef)==0){ # code for force weight when fit is get from SupearLearner
n.learners <- length(lib) # number of learners
fit$coef <- rep(1/n.learners,n.learners)
}
} # only force weight to be one if there is only one algorithm in lib
.force_weight <- function(fit, model, lib){
if (length(lib)==1){ # only force weight to be one if there is only one algorithm in lib
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- length(fit$coef) # number of folds
fit$coef <- rep(1, K)
for (i in 1:K){
fit$AllSL[[i]]$coef <- 1
}
}else if (model=="sl"){ # code for force weight when fit is get from SupearLearner
fit$coef <- 1
}
} # only force weight to be one if there is only one algorithm in lib
if (length(lib)>1){ # if there are more than one learner, and the coefficient for all learners are 0
if (model=="cv"){ # code for force weight when fit is get from CV.SupearLearner
K <- nrow(fit$coef) # number of folds
n.learners <- ncol(fit$coef) # number of learners
for (i in 1:K){
if (sum(fit$AllSL[[i]]$coef)==0){ # if the coefficient for all learners are 0, then force equal weights to all learners
fit$AllSL[[i]]$coef <- rep(1/n.learners,n.learners)
fit$coef[i,] <- rep(1/n.learners,n.learners)
}
}
}else if (model=="sl" & sum(fit$coef)==0){ # code for force weight when fit is get from SupearLearner
n.learners <- length(lib) # number of learners
fit$coef <- rep(1/n.learners,n.learners)
}
} # only force weight to be one if there is only one algorithm in lib
return(fit)
}
or_fit <- .force_weight(or_fit, model="cv", lib=lib.Y) # force weight to be 1 for the one algorithm
devtools::check()
devtools::build()
eval(parse('list(M.1 = \"M.1 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)\", M.2 = \"M.2 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)\")'))
eval(parse("list(M.1 = \"M.1 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)\", M.2 = \"M.2 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(A*X.1) + I(A*X.2) + I(A*X.3) + I(A*X.4) + I(A*X.5)+ I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)\")"))
eval(parse('list(c(\"A\", \"L\"), c(\"M\", \"Y\"))'))
eval(parse('list(c("A", "L"), c("M", "Y"))'))
eval(parse(list(c("A", "L"), c("M", "Y"))))
# Corrected string with properly escaped quotes
input_string <- 'list(
M.1 = "M.1 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 +
I(A * X.1) + I(A * X.2) + I(A * X.3) + I(A * X.4) + I(A * X.5) +
I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)",
M.2 = "M.2 ~ A + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 +
I(A * X.1) + I(A * X.2) + I(A * X.3) + I(A * X.4) + I(A * X.5) +
I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2)"
)'
# Convert the string into an R expression and evaluate it
evaluated_list <- eval(parse(text = input_string))
# Print the result to verify
print(evaluated_list)
str <- 'list(L = "L ~ M.1 + M.2 + X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + I(X.5^2) + I(X.6^2) + I(X.7^2) + I(X.8^2) + I(X.9^2) + I(X.10^2) + A + I(A*X.1)" )'
evaluated_list <- eval(parse(text = str))
eval(parse('list(c("A", "L"), c("M", "Y"))'))
eval(parse(text='list(c("A", "L"), c("M", "Y"))'))
?parse
library("devtools")
library("roxygen2")
devtools::install()
library(ADMGtmle)
graph <- make.graph(vertices=c('A','M','L','Y','X'),
bi_edges=list(c('A','L'), c('Y','L')),
di_edges=list(c('X','A'), c('X','M'), c('X','L'),
c('X','Y'), c('A','M'), c('M','L'), c('L','Y'), c('M','Y')))
is.np.saturated(graph)
is.mb.shielded(graph)
graph <- make.graph(vertices=c('A','M','L','Y','X'),
bi_edges=list(c('A','L'), c('L','Y')),
di_edges=list(c('X','A'), c('X','M'), c('X','L'),
c('X','Y'), c('A','M'), c('M','L'), c('L','Y'), c('M','Y')))
is.np.saturated(graph)
graph <- make.graph(vertices=c('A','M','L','Y','X'),
bi_edges=list(c('A','L'), c('L','Y')),
di_edges=list(c('X','A'), c('X','M'), c('X','L'),
c('X','Y'), c('A','M'), c('M','L'), c('L','Y'), c('M','Y')))
# get the topological ordering of the graph
top_order <- f.top_order(graph)
#' @return A logical value indicating whether the graph is nonparametrically saturated.
#' @export
#' @importFrom utils combn
#' @examples
#' graph <- make.graph(vertices=c('A','M','L','Y','X'),
#' bi_edges=list(c('A','Y')),
#' di_edges=list(c('X','A'), c('X','M'), c('X','L'),
#' c('X','Y'), c('M','Y'), c('A','M'), c('A','L'), c('M','L'), c('L','Y')))
#' is.np.saturated(graph)
#'
is.np.saturated <- function(graph) {
# get the topological ordering of the graph
top_order <- f.top_order(graph)
# Iterate over all pairs of vertices
for (pair in combn(graph$vertices, 2, simplify = FALSE)) {
Vi <- pair[1]
Vj <- pair[2]
# order Vi and Vj
if (which(top_order == Vi) > which(top_order == Vj)) {
V1 <- Vj
V2 <- Vi
} else {
V1 <- Vi
V2 <- Vj
}
# Check if there is no dense inducing path between Vi and Vj
# 1. V1 is not in the parents set of Di for all Di in D, where D is the district of V2 in conditional acyclic directed mixed graphs (CADMG) obtained by recursively fixing as many vertices as possible in V/V2
# AND 2. The CADMG obtained by recursively fixing as many vertices as possible in V/{Vi, Vj} has more than one district.
# (cnt.districts(f.reachable_closure(graph,c(Vi,Vj))[[3]])$n.districts > 1)
# !(V1 %in% f.district(f.reachable_closure(graph,c(V1,V2))[[3]], V2))
if (!(V1 %in% f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2))) &&  (cnt.districts(f.reachable_closure(graph,c(Vi,Vj))[[3]])$n.districts > 1)){
message("The graph is not nonparametrically saturated.")
print(c(V1,V2))
return(FALSE)
} # end of if statement
} # end of for loop
message("The graph is nonparametrically saturated.")
return(TRUE)
}
is.np.saturated(graph)
graph <- make.graph(vertices=c('A','M','L','Y','X'),
bi_edges=list(c('A','L'), c('L','Y')),
di_edges=list(c('X','A'), c('X','M'), c('X','L'),c('X','Y'), c('A','M'), c('M','L'), c('L','Y'), c('M','Y')))
is.np.saturated(graph)
combn(graph$vertices, 2, simplify = FALSE)
(cnt.districts(f.reachable_closure(graph,c('A','Y'))[[3]])$n.districts > 1)
!(V1 %in% f.district(f.reachable_closure(graph,c(V1,V2))[[3]], V2))
!('A' %in% f.district(f.reachable_closure(graph,c(V1,V2))[[3]], 'Y'))
!('A' %in% f.district(f.reachable_closure(graph,c('A','Y'))[[3]], 'Y'))
V1 = 'A'
V2='Y'
!(V1 %in% f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2))) &&  (cnt.districts(f.reachable_closure(graph,c(Vi,Vj))[[3]])$n.districts > 1)
Vi='A'
Vj='Y'
!(V1 %in% f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2))) &&  (cnt.districts(f.reachable_closure(graph,c(Vi,Vj))[[3]])$n.districts > 1)
!(V1 %in% f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2)))
f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2)))
f.parents(graph, f.district(f.reachable_closure(graph,V2)[[3]], V2))
cnt.districts(f.reachable_closure(graph,c(Vi,Vj))[[3]])$n.districts
f.reachable_closure(graph,c(Vi,Vj))
